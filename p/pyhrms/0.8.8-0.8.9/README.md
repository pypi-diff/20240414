# Comparing `tmp/pyhrms-0.8.8.zip` & `tmp/pyhrms-0.8.9.zip`

## zipinfo {}

```diff
@@ -1,16 +1,16 @@
-Zip file size: 101077 bytes, number of entries: 14
-drwxr-xr-x  2.0 unx        0 b- stor 24-Mar-25 22:16 pyhrms-0.8.8/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Mar-25 22:16 pyhrms-0.8.8/pyhrms.egg-info/
-drwxr-xr-x  2.0 unx        0 b- stor 24-Mar-25 22:16 pyhrms-0.8.8/pyhrms/
--rw-r--r--  2.0 unx    29477 b- defN 24-Mar-25 22:16 pyhrms-0.8.8/PKG-INFO
--rwx------  2.0 unx      927 b- defN 24-Mar-25 22:14 pyhrms-0.8.8/setup.py
--rw-r--r--  2.0 unx       38 b- defN 24-Mar-25 22:16 pyhrms-0.8.8/setup.cfg
--rwx------  2.0 unx    24979 b- defN 24-Mar-25 22:14 pyhrms-0.8.8/README.rst
--rw-r--r--  2.0 unx    29477 b- defN 24-Mar-25 22:16 pyhrms-0.8.8/pyhrms.egg-info/PKG-INFO
--rw-r--r--  2.0 unx      204 b- defN 24-Mar-25 22:16 pyhrms-0.8.8/pyhrms.egg-info/SOURCES.txt
--rw-r--r--  2.0 unx      181 b- defN 24-Mar-25 22:16 pyhrms-0.8.8/pyhrms.egg-info/requires.txt
--rw-r--r--  2.0 unx        7 b- defN 24-Mar-25 22:16 pyhrms-0.8.8/pyhrms.egg-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 24-Mar-25 22:16 pyhrms-0.8.8/pyhrms.egg-info/dependency_links.txt
--rwx------  2.0 unx   305502 b- defN 24-Mar-25 21:47 pyhrms-0.8.8/pyhrms/pyhrms.py
--rwx------  2.0 unx     2199 b- defN 24-Mar-25 21:53 pyhrms-0.8.8/pyhrms/__init__.py
-14 files, 392992 bytes uncompressed, 99155 bytes compressed:  74.8%
+Zip file size: 101898 bytes, number of entries: 14
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-14 23:13 pyhrms-0.8.9/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-14 23:13 pyhrms-0.8.9/pyhrms.egg-info/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-14 23:13 pyhrms-0.8.9/pyhrms/
+-rw-r--r--  2.0 unx    30087 b- defN 24-Apr-14 23:13 pyhrms-0.8.9/PKG-INFO
+-rwx------  2.0 unx      927 b- defN 24-Apr-14 23:08 pyhrms-0.8.9/setup.py
+-rw-r--r--  2.0 unx       38 b- defN 24-Apr-14 23:13 pyhrms-0.8.9/setup.cfg
+-rwx------  2.0 unx    25541 b- defN 24-Apr-14 23:12 pyhrms-0.8.9/README.rst
+-rw-r--r--  2.0 unx    30087 b- defN 24-Apr-14 23:13 pyhrms-0.8.9/pyhrms.egg-info/PKG-INFO
+-rw-r--r--  2.0 unx      204 b- defN 24-Apr-14 23:13 pyhrms-0.8.9/pyhrms.egg-info/SOURCES.txt
+-rw-r--r--  2.0 unx      181 b- defN 24-Apr-14 23:13 pyhrms-0.8.9/pyhrms.egg-info/requires.txt
+-rw-r--r--  2.0 unx        7 b- defN 24-Apr-14 23:13 pyhrms-0.8.9/pyhrms.egg-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 24-Apr-14 23:13 pyhrms-0.8.9/pyhrms.egg-info/dependency_links.txt
+-rwx------  2.0 unx   308332 b- defN 24-Apr-14 23:07 pyhrms-0.8.9/pyhrms/pyhrms.py
+-rwx------  2.0 unx     2199 b- defN 24-Apr-14 23:07 pyhrms-0.8.9/pyhrms/__init__.py
+14 files, 397604 bytes uncompressed, 99976 bytes compressed:  74.9%
```

## zipnote {}

```diff
@@ -1,43 +1,43 @@
-Filename: pyhrms-0.8.8/
+Filename: pyhrms-0.8.9/
 Comment: 
 
-Filename: pyhrms-0.8.8/pyhrms.egg-info/
+Filename: pyhrms-0.8.9/pyhrms.egg-info/
 Comment: 
 
-Filename: pyhrms-0.8.8/pyhrms/
+Filename: pyhrms-0.8.9/pyhrms/
 Comment: 
 
-Filename: pyhrms-0.8.8/PKG-INFO
+Filename: pyhrms-0.8.9/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.8.8/setup.py
+Filename: pyhrms-0.8.9/setup.py
 Comment: 
 
-Filename: pyhrms-0.8.8/setup.cfg
+Filename: pyhrms-0.8.9/setup.cfg
 Comment: 
 
-Filename: pyhrms-0.8.8/README.rst
+Filename: pyhrms-0.8.9/README.rst
 Comment: 
 
-Filename: pyhrms-0.8.8/pyhrms.egg-info/PKG-INFO
+Filename: pyhrms-0.8.9/pyhrms.egg-info/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.8.8/pyhrms.egg-info/SOURCES.txt
+Filename: pyhrms-0.8.9/pyhrms.egg-info/SOURCES.txt
 Comment: 
 
-Filename: pyhrms-0.8.8/pyhrms.egg-info/requires.txt
+Filename: pyhrms-0.8.9/pyhrms.egg-info/requires.txt
 Comment: 
 
-Filename: pyhrms-0.8.8/pyhrms.egg-info/top_level.txt
+Filename: pyhrms-0.8.9/pyhrms.egg-info/top_level.txt
 Comment: 
 
-Filename: pyhrms-0.8.8/pyhrms.egg-info/dependency_links.txt
+Filename: pyhrms-0.8.9/pyhrms.egg-info/dependency_links.txt
 Comment: 
 
-Filename: pyhrms-0.8.8/pyhrms/pyhrms.py
+Filename: pyhrms-0.8.9/pyhrms/pyhrms.py
 Comment: 
 
-Filename: pyhrms-0.8.8/pyhrms/__init__.py
+Filename: pyhrms-0.8.9/pyhrms/__init__.py
 Comment: 
 
 Zip file comment:
```

## Comparing `pyhrms-0.8.8/PKG-INFO` & `pyhrms-0.8.9/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 1.2
 Name: pyhrms
-Version: 0.8.8
+Version: 0.8.9
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Description: PyHRMS: Tools For working with High Resolution Mass Spectrometry (HRMS) data in Environmental Science
         =====================================================================================================
@@ -15,19 +15,23 @@
         
         Contributer: Rui Wang
         ======================
         First release date: Nov.15.2021
         
         Update
         ======
-        Mar.25.2024: pyhrms 0.8.8 new features:
+        Apr.14.2024: pyhrms 0.8.9 new features:
         
-            * update gen_DDA_ms2_df
+            * Corrected issues in rt_matching and ms2_matching to prevent errors;
         
-            * add functions to process AIF MS2 data
+            * Updated gen_DDA_ms2_df to include isotope information
+        
+            * Improved the peak_checking_area_precursor_frag_swath function with various bug fixes.
+        
+            * Enhanced the swath_process function with new features to mark isotopes and adducts
         
         pyhrms can be installed and import as following:
         
         .. code-block:: python
         
             pip install pyhrms
         
@@ -86,14 +90,16 @@
         
         * Song, D., Tang, T., Wang, R., Liu, H., Xie, D., Zhao, B., Dang, Z., Lu, G., 2024. Enhancing compound confidence in suspect and non-target screening through machine learning-based retention time prediction. Environ. Pollut. 347, 123763. https://doi.org/10.1016/j.envpol.2024.123763
         
         * Chen, J., Tang, T., Li, Y., Wang, R., Chen, X., Song, D., Du, X., Tao, X., Zhou, J., Dang, Z., Lu, G., 2024. Non-targeted screening and photolysis transformation of tire-related compounds in roadway runoff. Sci. Total. Environ. 924, 171622. https://doi.org/10.1016/j.scitotenv.2024.171622
         
         * Xia,D., Liu, L., Zhao, B., Xie,D., Lu, G., and Wang, R., Application of Nontarget High-Resolution Mass Spectrometry Fingerprints for Qualitative and Quantitative Source Apportionment: A Real Case Study.Environ. Sci.Technol. 2024, 58,727−738 https://doi.org/10.1021/acs.est.3c06688
         
+        * Jia, W., Liu, H., Ma, Y., Huang, G., Liu, Y., Zhao, B., Xie, D., Huang, K., Wang, R., 2024. Reproducibility in nontarget screening (NTS) of environmental emerging contaminants: Assessing different HLB SPE cartridges and instruments. Sci. Total Environ. 912, 168971. https://doi.org/10.1016/j.scitotenv.2023.168971
+        
         * Wang, R., Yan, Y., Liu, H., Li, Yanxi, Jin, M., Li, Yuqing, Tao, R., Chen, Q., Wang, X., Zhao, B., Xie, D., 2023. Integrating data dependent and data independent non-target screening methods for monitoring emerging contaminants in the Pearl River of Guangdong Province, China. Sci. Total Environ. 891, 164445. https://doi.org/10.1016/j.scitotenv.2023.164445
         
         * Jiang, X., Xue, Z., Chen, W., Xu, M., Liu, H., Liang, J., Zhang, L., Sun, Y., Liu, C., Yang, X., 2023. Biotransformation kinetics and pathways of typical synthetic progestins in soil microcosms. J. Hazard. Mater. 446, 130684. https://doi.org/10.1016/j.jhazmat.2022.130684
         
         * Liang, J., Wang, R., Liu, H., Xie, D., Tao, X., Zhou, J., Yin, H., Dang, Z., Lu, G., 2022. Unintentional formation of mixed chloro-bromo diphenyl ethers (PBCDEs), dibenzo-p-dioxins and dibenzofurans (PBCDD/Fs) from pyrolysis of polybrominated diphenyl ethers (PBDEs). Chemosphere 308, 136246. https://doi.org/10.1016/j.chemosphere.2022.136246
         
         * Xia, D., Liu, H., Lu, Y., Liu, Y., Liang, J., Xie, D., Lu, G., Qiu, J., Wang, R., 2023. Utility of a non-target screening method to explore the chlorination of similar sulfonamide antibiotics: Pathways and N Cl intermediates. Sci. Total Environ. 858, 160042. https://doi.org/10.1016/j.scitotenv.2022.160042
```

## Comparing `pyhrms-0.8.8/setup.py` & `pyhrms-0.8.9/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 def readme_file():
     with open('README.rst') as rf:
         return rf.read()
 
 setuptools.setup(
     name = 'pyhrms',
-    version = '0.8.8',
+    version = '0.8.9',
     author = 'Wang Rui',
     author_email = 'wtrt7009@gmail.com',
     url = 'https://github.com/WangRui5/PyHRMS.git',
     description = 'A powerful GC/LC-HRMS data analysis tool',
     long_description = readme_file(),
     packages = setuptools.find_packages(),
     install_requires = ['numpy>=1.19.2','pandas>=1.3.3'
```

## Comparing `pyhrms-0.8.8/README.rst` & `pyhrms-0.8.9/README.rst`

 * *Files 1% similar despite different names*

```diff
@@ -7,19 +7,23 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-Mar.25.2024: pyhrms 0.8.8 new features:
+Apr.14.2024: pyhrms 0.8.9 new features:
 
-    * update gen_DDA_ms2_df
+    * Corrected issues in rt_matching and ms2_matching to prevent errors;
 
-    * add functions to process AIF MS2 data
+    * Updated gen_DDA_ms2_df to include isotope information
+
+    * Improved the peak_checking_area_precursor_frag_swath function with various bug fixes.
+
+    * Enhanced the swath_process function with new features to mark isotopes and adducts
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
 
     pip install pyhrms
 
@@ -78,14 +82,16 @@
 
 * Song, D., Tang, T., Wang, R., Liu, H., Xie, D., Zhao, B., Dang, Z., Lu, G., 2024. Enhancing compound confidence in suspect and non-target screening through machine learning-based retention time prediction. Environ. Pollut. 347, 123763. https://doi.org/10.1016/j.envpol.2024.123763
 
 * Chen, J., Tang, T., Li, Y., Wang, R., Chen, X., Song, D., Du, X., Tao, X., Zhou, J., Dang, Z., Lu, G., 2024. Non-targeted screening and photolysis transformation of tire-related compounds in roadway runoff. Sci. Total. Environ. 924, 171622. https://doi.org/10.1016/j.scitotenv.2024.171622
 
 * Xia,D., Liu, L., Zhao, B., Xie,D., Lu, G., and Wang, R., Application of Nontarget High-Resolution Mass Spectrometry Fingerprints for Qualitative and Quantitative Source Apportionment: A Real Case Study.Environ. Sci.Technol. 2024, 58,727−738 https://doi.org/10.1021/acs.est.3c06688
 
+* Jia, W., Liu, H., Ma, Y., Huang, G., Liu, Y., Zhao, B., Xie, D., Huang, K., Wang, R., 2024. Reproducibility in nontarget screening (NTS) of environmental emerging contaminants: Assessing different HLB SPE cartridges and instruments. Sci. Total Environ. 912, 168971. https://doi.org/10.1016/j.scitotenv.2023.168971
+
 * Wang, R., Yan, Y., Liu, H., Li, Yanxi, Jin, M., Li, Yuqing, Tao, R., Chen, Q., Wang, X., Zhao, B., Xie, D., 2023. Integrating data dependent and data independent non-target screening methods for monitoring emerging contaminants in the Pearl River of Guangdong Province, China. Sci. Total Environ. 891, 164445. https://doi.org/10.1016/j.scitotenv.2023.164445
 
 * Jiang, X., Xue, Z., Chen, W., Xu, M., Liu, H., Liang, J., Zhang, L., Sun, Y., Liu, C., Yang, X., 2023. Biotransformation kinetics and pathways of typical synthetic progestins in soil microcosms. J. Hazard. Mater. 446, 130684. https://doi.org/10.1016/j.jhazmat.2022.130684
 
 * Liang, J., Wang, R., Liu, H., Xie, D., Tao, X., Zhou, J., Yin, H., Dang, Z., Lu, G., 2022. Unintentional formation of mixed chloro-bromo diphenyl ethers (PBCDEs), dibenzo-p-dioxins and dibenzofurans (PBCDD/Fs) from pyrolysis of polybrominated diphenyl ethers (PBDEs). Chemosphere 308, 136246. https://doi.org/10.1016/j.chemosphere.2022.136246
 
 * Xia, D., Liu, H., Lu, Y., Liu, Y., Liang, J., Xie, D., Lu, G., Qiu, J., Wang, R., 2023. Utility of a non-target screening method to explore the chlorination of similar sulfonamide antibiotics: Pathways and N Cl intermediates. Sci. Total Environ. 858, 160042. https://doi.org/10.1016/j.scitotenv.2022.160042
```

## Comparing `pyhrms-0.8.8/pyhrms.egg-info/PKG-INFO` & `pyhrms-0.8.9/pyhrms.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 1.2
 Name: pyhrms
-Version: 0.8.8
+Version: 0.8.9
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Description: PyHRMS: Tools For working with High Resolution Mass Spectrometry (HRMS) data in Environmental Science
         =====================================================================================================
@@ -15,19 +15,23 @@
         
         Contributer: Rui Wang
         ======================
         First release date: Nov.15.2021
         
         Update
         ======
-        Mar.25.2024: pyhrms 0.8.8 new features:
+        Apr.14.2024: pyhrms 0.8.9 new features:
         
-            * update gen_DDA_ms2_df
+            * Corrected issues in rt_matching and ms2_matching to prevent errors;
         
-            * add functions to process AIF MS2 data
+            * Updated gen_DDA_ms2_df to include isotope information
+        
+            * Improved the peak_checking_area_precursor_frag_swath function with various bug fixes.
+        
+            * Enhanced the swath_process function with new features to mark isotopes and adducts
         
         pyhrms can be installed and import as following:
         
         .. code-block:: python
         
             pip install pyhrms
         
@@ -86,14 +90,16 @@
         
         * Song, D., Tang, T., Wang, R., Liu, H., Xie, D., Zhao, B., Dang, Z., Lu, G., 2024. Enhancing compound confidence in suspect and non-target screening through machine learning-based retention time prediction. Environ. Pollut. 347, 123763. https://doi.org/10.1016/j.envpol.2024.123763
         
         * Chen, J., Tang, T., Li, Y., Wang, R., Chen, X., Song, D., Du, X., Tao, X., Zhou, J., Dang, Z., Lu, G., 2024. Non-targeted screening and photolysis transformation of tire-related compounds in roadway runoff. Sci. Total. Environ. 924, 171622. https://doi.org/10.1016/j.scitotenv.2024.171622
         
         * Xia,D., Liu, L., Zhao, B., Xie,D., Lu, G., and Wang, R., Application of Nontarget High-Resolution Mass Spectrometry Fingerprints for Qualitative and Quantitative Source Apportionment: A Real Case Study.Environ. Sci.Technol. 2024, 58,727−738 https://doi.org/10.1021/acs.est.3c06688
         
+        * Jia, W., Liu, H., Ma, Y., Huang, G., Liu, Y., Zhao, B., Xie, D., Huang, K., Wang, R., 2024. Reproducibility in nontarget screening (NTS) of environmental emerging contaminants: Assessing different HLB SPE cartridges and instruments. Sci. Total Environ. 912, 168971. https://doi.org/10.1016/j.scitotenv.2023.168971
+        
         * Wang, R., Yan, Y., Liu, H., Li, Yanxi, Jin, M., Li, Yuqing, Tao, R., Chen, Q., Wang, X., Zhao, B., Xie, D., 2023. Integrating data dependent and data independent non-target screening methods for monitoring emerging contaminants in the Pearl River of Guangdong Province, China. Sci. Total Environ. 891, 164445. https://doi.org/10.1016/j.scitotenv.2023.164445
         
         * Jiang, X., Xue, Z., Chen, W., Xu, M., Liu, H., Liang, J., Zhang, L., Sun, Y., Liu, C., Yang, X., 2023. Biotransformation kinetics and pathways of typical synthetic progestins in soil microcosms. J. Hazard. Mater. 446, 130684. https://doi.org/10.1016/j.jhazmat.2022.130684
         
         * Liang, J., Wang, R., Liu, H., Xie, D., Tao, X., Zhou, J., Yin, H., Dang, Z., Lu, G., 2022. Unintentional formation of mixed chloro-bromo diphenyl ethers (PBCDEs), dibenzo-p-dioxins and dibenzofurans (PBCDD/Fs) from pyrolysis of polybrominated diphenyl ethers (PBDEs). Chemosphere 308, 136246. https://doi.org/10.1016/j.chemosphere.2022.136246
         
         * Xia, D., Liu, H., Lu, Y., Liu, Y., Liang, J., Xie, D., Lu, G., Qiu, J., Wang, R., 2023. Utility of a non-target screening method to explore the chlorination of similar sulfonamide antibiotics: Pathways and N Cl intermediates. Sci. Total Environ. 858, 160042. https://doi.org/10.1016/j.scitotenv.2022.160042
```

## Comparing `pyhrms-0.8.8/pyhrms/pyhrms.py` & `pyhrms-0.8.9/pyhrms/pyhrms.py`

 * *Files 3% similar despite different names*

```diff
@@ -25,34 +25,33 @@
 import bisect
 import re
 from collections import defaultdict
 import requests
 from bs4 import BeautifulSoup
 import networkx as nx
 
-
 """
 ========================================================================================================
 1. basic function
 ========================================================================================================
 """
 
-
 atom_mass_table = pd.Series(
     data={'C': 12.000000, 'Ciso': 13.003355, 'N': 14.003074, 'Niso': 15.000109, 'O': 15.994915, 'H': 1.007825,
           'Oiso': 17.999159, 'F': 18.998403, 'K': 38.963708, 'P': 30.973763, 'Cl': 34.968853,
           'Cliso': 36.965903, 'S': 31.972072, 'Siso': 33.967868, 'Br': 78.918336, 'Na': 22.989770,
           'Si': 27.976928, 'Fe': 55.934939, 'Se': 79.916521, 'As': 74.921596, 'I': 126.904477, 'D': 2.014102,
           'Co': 58.933198, 'Au': 196.966560, 'B': 11.009305, 'e': 0.0005486, 'Cr': 51.996, 'Sn': 111.904826,
           'Ag': 106.905095, 'Hg': 195.965812, 'Li': 6.015123
           })
 
 
 def one_step_process(path, company, profile=True, control_group=['lab_blank', 'methanol'], ms2_analysis=True,
-                     peak_width=2,threshold = 15, filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1, orbi=False):
+                     peak_width=2, threshold=15, filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1,
+                     orbi=False):
     """
     This function using one processor to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
        - control_group (List[str]): A list of labels representing the control group.These labels are used in the search for relevant file names.
@@ -77,18 +76,18 @@
     print('First process...')
     print('============================================================================')
     print('                                                                            ')
 
     files_mzml = glob(os.path.join(path, '*.mzML'))
     files_mzml_DDA = [file for file in files_mzml if 'DDA' in os.path.basename(file)]
     files_mzml = [file for file in files_mzml if 'DDA' not in os.path.basename(file)]
-    for j,file in enumerate(files_mzml):
+    for j, file in enumerate(files_mzml):
         first_process(file, company=company, profile=profile, control_group=control_group, ms2_analysis=ms2_analysis,
-                      peak_width=peak_width,threshold = threshold, split_n=split_n, sat_intensity=sat_intensity,
-                      long_rt_split_n=long_rt_split_n, orbi=orbi, message = f'No. {j+1} : ')
+                      peak_width=peak_width, threshold=threshold, split_n=split_n, sat_intensity=sat_intensity,
+                      long_rt_split_n=long_rt_split_n, orbi=orbi, message=f'No. {j + 1} : ')
 
     # 检查是否有遗漏的
     files_excel_temp = glob(os.path.join(path, '*.xlsx'))
     files_excel_names = [os.path.basename(i)[:-5] for i in files_excel_temp]
     path_omitted = []
     if len(files_mzml) > len(files_excel_names):
         # 检查是哪个文件漏掉了
@@ -98,51 +97,47 @@
             else:
                 path_omitted.append(path1)
     if len(path_omitted) == 0:
         pass
     else:
         for file in path_omitted:
             first_process(file, company=company, profile=profile, control_group=control_group,
-                          ms2_analysis=ms2_analysis, peak_width=peak_width,threshold = threshold,
+                          ms2_analysis=ms2_analysis, peak_width=peak_width, threshold=threshold,
                           split_n=split_n, sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n, orbi=orbi)
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
-    
 
-    
     # 第二个过程
     print('                                                                            ')
     print('============================================================================')
     print('Second process...')
     print('============================================================================')
     print('                                                                            ')
-    for j,file in enumerate(files_mzml):
-        second_process(file, ref_all, company, profile=profile, long_rt_split_n=long_rt_split_n, orbi=orbi,message = f'No. {j+1} ')
-    
+    for j, file in enumerate(files_mzml):
+        second_process(file, ref_all, company, profile=profile, long_rt_split_n=long_rt_split_n, orbi=orbi,
+                       message=f'No. {j + 1} ')
 
-    
-    
     # 第三个过程, 做fold change filter
     print('                                                                            ')
     print('============================================================================')
     print('Third process...')
     print('============================================================================')
     print('                                                                            ')
 
     fold_change_filter(path, control_group=control_group, filter_type=filter_type)
 
     # 如果有DDA，将DDA数据加入到excel里
-    DDA_to_DIA_result(path,company,profile)
+    DDA_to_DIA_result(path, company, profile)
 
 
-                
-def one_step_process_DDA(path,company, profile = True,i_threshold = 200, orbi = False,control_group = ['methanol'],filter_type = 1):
+def one_step_process_DDA(path, company, profile=True, i_threshold=200, orbi=False, control_group=['methanol'],
+                         filter_type=1):
     """
     Only process DDA data. This function using one processor to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
        - control_group (List[str]): A list of labels representing the control group.These labels are used in the search for relevant file names.
@@ -154,44 +149,44 @@
                            and fold change is computed as the ratio of the mean sample area
                            to the mean control area.
        - orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False)
     returns:
         None.Generate Excel files that summarizes the differences between the control sets and sample sets.
 
     """
-    
-    
-    files_mzml = glob(os.path.join(path,'*.mzML'))
-    
+
+    files_mzml = glob(os.path.join(path, '*.mzML'))
+
     # 第一步，生成文件
-    for i,file in enumerate(files_mzml):
-        message = f'No. {i+1}: '
-        ms1, ms2 = sep_scans(file, company, message = message)
-        DDA_df = gen_DDA_ms2_df(ms1,ms2,i_threshold=i_threshold,profile = profile, opt=False,more_info= True,message = message)
+    for i, file in enumerate(files_mzml):
+        message = f'No. {i + 1}: '
+        ms1, ms2 = sep_scans(file, company, message=message)
+        DDA_df = gen_DDA_ms2_df(ms1, ms2, i_threshold=i_threshold, profile=profile, opt=False, more_info=True,
+                                message=message)
         DDA_df['mz'] = DDA_df['precursor']
-        DDA_df = DDA_df[DDA_df['frag'].apply(len) !=0].reset_index(drop = True)
-        DDA_df.to_excel(file.replace('.mzML','_DDA_info.xlsx'))
-    
+        DDA_df = DDA_df[DDA_df['frag'].apply(len) != 0].reset_index(drop=True)
+        DDA_df.to_excel(file.replace('.mzML', '_DDA_info.xlsx'))
+
     # 做alignment
-    files_excel = glob(os.path.join(path,'*.xlsx'))
-    peak_alignment(files_excel,rt_error = 0.1,mz_error = 0.015)
-    ref_all = pd.read_excel(os.path.join(path,r'peak_ref.xlsx'))
+    files_excel = glob(os.path.join(path, '*.xlsx'))
+    peak_alignment(files_excel, rt_error=0.1, mz_error=0.015)
+    ref_all = pd.read_excel(os.path.join(path, r'peak_ref.xlsx'))
     # 第二步骤
-    for i,file in enumerate(files_mzml):
-        message = f'No. {i+1}: '
-        second_process(file, ref_all,company,profile = profile,orbi = orbi, message = message)
-    
+    for i, file in enumerate(files_mzml):
+        message = f'No. {i + 1}: '
+        second_process(file, ref_all, company, profile=profile, orbi=orbi, message=message)
+
     # 第三步
-    fold_change_filter(path,control_group=control_group,filter_type=filter_type)                
+    fold_change_filter(path, control_group=control_group, filter_type=filter_type)
+
 
 def ultimate_peak_picking(ms1, profile=True, split_n=20, threshold=15, i_threshold=500, peak_width=2,
-                          SN_threshold=3, noise_threshold = 0, rt_error_alignment=0.05,
+                          SN_threshold=3, noise_threshold=0, rt_error_alignment=0.05,
                           mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False, long_rt_split_n=1, rt_overlap=1,
-                          orbi=False,message = ''):
-
+                          orbi=False, message=''):
     """
     Find peaks in the orginal ms1 list, analyze isotope and adduct information, and return a dataframe with
     information on the peaks including retention time, m/z value, intensity, and area.
 
     Args:
         ms1 (scan list): generated from sep_scans(file.mzML).
         profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
@@ -207,22 +202,21 @@
         rt_overlap: The rt overlap (min) between adjacent sections of data when splitting it.
         orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False).
     Returns:
         pandas.DataFrame: A dataframe with information on the peaks including retention time, m/z value,
         intensity, and area.
     """
 
-
     if long_rt_split_n == 1:
         peak_all = split_peak_picking(ms1, profile=profile, split_n=split_n, threshold=threshold, peak_width=peak_width,
                                       i_threshold=i_threshold,
                                       SN_threshold=SN_threshold, noise_threshold=noise_threshold,
                                       rt_error_alignment=rt_error_alignment,
                                       mz_error_alignment=mz_error_alignment, mz_overlap=mz_overlap,
-                                      sat_intensity=sat_intensity, orbi=orbi,message=message)
+                                      sat_intensity=sat_intensity, orbi=orbi, message=message)
     else:
         # Calculate the length of each part
         total_spectra = len(ms1)
         part_length = total_spectra // long_rt_split_n
         overlap_spectra = int(rt_overlap / (ms1[1].scan_time[0] - ms1[0].scan_time[
             0]))  # calculate the number of spectra in 1 minute of retention time
 
@@ -254,26 +248,26 @@
         ranges = [[my_list[i][0], my_list[i + 1][0]] for i in range(len(my_list) - 1)] + [my_list[-1]]
 
         # start to do peak picking for each part
         peak_list_all = []
         for n, part in enumerate(parts):
             peak_all = split_peak_picking(part, profile=profile, i_threshold=i_threshold, peak_width=peak_width,
                                           SN_threshold=SN_threshold, split_n=split_n, sat_intensity=sat_intensity,
-                                          orbi=orbi,message=message)
+                                          orbi=orbi, message=message)
             peak_all = peak_all[(peak_all['rt'] > ranges[n][0]) & (peak_all['rt'] <= ranges[n][1])]
             peak_list_all.append(peak_all)
 
         peak_all = pd.concat(peak_list_all).reset_index(drop=True)
     return peak_all
 
 
 def first_process(file, company, profile=True, control_group=['methanol_blank', 'control', 'lab_blank'],
-                  i_threshold=200, SN_threshold=3, peak_width=2,threshold=15,
+                  i_threshold=200, SN_threshold=3, peak_width=2, threshold=15,
                   ms2_analysis=True, frag_rt_error=0.02, split_n=20, sat_intensity=False, long_rt_split_n=1,
-                  orbi=False, message = ''):
+                  orbi=False, message=''):
     """
     Processes HRMS data by performing peak picking and generating a result file.
 
     Args:
         file (str): Path to the input file to be processed.
         company (str): The manufacturer of the instrument used to generate the data (e.g., 'Waters', 'Agilent', etc.).
         profile (bool): A flag indicating whether or not to perform profiling of chromatographic peaks.
@@ -288,38 +282,39 @@
         long_rt_split_n: The number of pieces to split the ms1.
         orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False)
 
     Returns:
         None. Instead, the function exports an Excel file with the result information.
     """
     mz_round = 4
-    ms1, ms2 = sep_scans(file, company,message = message)
+    ms1, ms2 = sep_scans(file, company, message=message)
     peak_all = ultimate_peak_picking(ms1, profile=profile, split_n=split_n, i_threshold=i_threshold,
-                                     peak_width=peak_width,threshold=threshold,
+                                     peak_width=peak_width, threshold=threshold,
                                      SN_threshold=SN_threshold, sat_intensity=sat_intensity,
-                                     long_rt_split_n=long_rt_split_n, orbi=orbi,message=message)
+                                     long_rt_split_n=long_rt_split_n, orbi=orbi, message=message)
 
     # 是否分析ms2
     if len(ms2) == 0:
         pass
     else:
         if ms2_analysis is True:
             basename_file = os.path.basename(file)
             if any(item.lower() in basename_file.lower() for item in control_group):
                 pass
             else:
 
                 peak_all2 = ultimate_peak_picking(ms2, profile=profile, split_n=split_n, i_threshold=i_threshold,
-                                                  peak_width=peak_width,threshold=threshold,
+                                                  peak_width=peak_width, threshold=threshold,
                                                   SN_threshold=SN_threshold, sat_intensity=sat_intensity,
-                                                  long_rt_split_n=long_rt_split_n, orbi=orbi,message=message)
+                                                  long_rt_split_n=long_rt_split_n, orbi=orbi, message=message)
 
                 frag_all = []
                 spec_all = []
-                for i in tqdm(range(len(peak_all)), desc=f'{message}Assign DIA MS2 spectrum',leave = False,colour = 'Green',ncols = 100):
+                for i in tqdm(range(len(peak_all)), desc=f'{message}Assign DIA MS2 spectrum', leave=False,
+                              colour='Green', ncols=100):
                     rt = peak_all.loc[i, 'rt']
                     df_DIA = peak_all2[(peak_all2['rt'] > rt - frag_rt_error)
                                        & (peak_all2['rt'] < rt + frag_rt_error)].sort_values(
                         by='intensity', ascending=False)
                     # append fragments
                     frag = str(list(df_DIA['mz'].values))
                     frag_all.append(frag)
@@ -360,17 +355,16 @@
             pass
     file_name = os.path.basename(file)
     peak_selected = identify_isotopes(peak_all)
     peak_selected = remove_unnamed_columns(peak_selected)
     peak_selected.to_excel(file.replace('.mzML', '.xlsx'))
 
 
-
 def multi_process(path, company, profile=True, control_group=['lab_blank', 'methanol'], processors=1, i_threshold=200,
-                  SN_threshold=3, peak_width=2, threshold = 15,ms2_analysis=True,
+                  SN_threshold=3, peak_width=2, threshold=15, ms2_analysis=True,
                   filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1, orbi=False):
     """
     This function is to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
@@ -396,21 +390,21 @@
     files_mzml_DDA = [file for file in files_mzml if 'DDA' in os.path.basename(file)]
     files_mzml = [file for file in files_mzml if 'DDA' not in os.path.basename(file)]
     # 第一个过程
     i_threshold = 200
     SN_threshold = 3,
     frag_rt_error = 0.02
     pool = Pool(processes=processors)
-    for j,file in enumerate(files_mzml):
+    for j, file in enumerate(files_mzml):
         print(file)
-        message = f'No. {j+1} : '
+        message = f'No. {j + 1} : '
         pool.apply_async(first_process,
-                         args=(file, company, profile, control_group, i_threshold, SN_threshold, peak_width,threshold,
+                         args=(file, company, profile, control_group, i_threshold, SN_threshold, peak_width, threshold,
                                ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n,
-                               orbi,message))
+                               orbi, message))
 
     print('                                                                            ')
     print('============================================================================')
     print('First process...')
     print('============================================================================')
     print('                                                                            ')
     pool.close()
@@ -431,30 +425,31 @@
         pass
     else:
         pool = Pool(processes=processors)
         for file in path_omitted:
             print('Omitted files')
             print(file)
             pool.apply_async(first_process,
-                             args=(file, company, profile, control_group, i_threshold, SN_threshold, peak_width,threshold,
-                                   ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n,
-                                   orbi))
+                             args=(
+                             file, company, profile, control_group, i_threshold, SN_threshold, peak_width, threshold,
+                             ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n,
+                             orbi))
         pool.close()
         pool.join()
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
 
     # 第二个过程
     pool = Pool(processes=processors)
-    for j,file in enumerate(files_mzml):
-        message = f'No. {j+1} : '
-        pool.apply_async(second_process, args=(file, ref_all, company, profile, long_rt_split_n, orbi,message))
+    for j, file in enumerate(files_mzml):
+        message = f'No. {j + 1} : '
+        pool.apply_async(second_process, args=(file, ref_all, company, profile, long_rt_split_n, orbi, message))
     print('                                                                            ')
     print('============================================================================')
     print('Second process...')
     print('============================================================================')
     print('                                                                            ')
     pool.close()
     pool.join()
@@ -465,18 +460,18 @@
     print('Third process...')
     print('============================================================================')
     print('                                                                            ')
 
     fold_change_filter(path, control_group=control_group, filter_type=filter_type)
 
     # 如果有DDA，将DDA数据加入到excel里
-    DDA_to_DIA_result(path,company,profile)
+    DDA_to_DIA_result(path, company, profile)
 
 
-def sep_scans(path, company, tool='pymzml', message = ''):
+def sep_scans(path, company, tool='pymzml', message=''):
     """
     Separates scans for MS1 and MS2 in mzML files using pymzml package.
     Args:
        path (str): The path of the mzML file.
        company (str): The instrument company name. Currently, supports 'Waters', 'Agilent', 'Thermo' or 'AB'.
        tool(str): pymzml or pyopenms
     Returns:
@@ -484,27 +479,27 @@
     """
     if tool == 'pymzml':
         if company.lower() == 'waters':
             # create a pymzml Reader object
             run = pymzml.run.Reader(path)
             ms1, ms2 = [], []
             # iterate over each scan in the mzML file
-            for scan in tqdm(run, desc=f'{message}Separating MS1 and MS2',leave = False):
+            for scan in tqdm(run, desc=f'{message}Separating MS1 and MS2', leave=False):
                 # extract function value from the scan's id_dict attribute
                 if scan.id_dict['function'] == 1:
                     ms1.append(scan)
                 elif scan.ms_level == 2:
                     ms2.append(scan)
                 else:
                     pass
             return ms1, ms2
         else:
             run = pymzml.run.Reader(path)
             ms1, ms2 = [], []
-            for scan in tqdm(run, desc=f'{message}Separating MS1 and MS2',leave = False):
+            for scan in tqdm(run, desc=f'{message}Separating MS1 and MS2', leave=False):
                 if scan.ms_level == 1:
                     ms1.append(scan)
                 elif scan.ms_level == 2:
                     ms2.append(scan)
                 else:
                     pass
             return ms1, ms2
@@ -827,16 +822,14 @@
             peak_all['mz_opt'] = mz_opt
             peak_all['resolution'] = resolution.astype(int)
             t4 = time.time()
             if enable_progress_bar:
                 print(f'{round(t4 - t3, 0)} s ')
             return peak_all
 
-        
-
 
 def isotope_distribution(spec1, mz, error=0.02):
     """
     Find the isotope distribution for a specific mass-to-charge ratio (m/z).
 
     Args:
     - spec1: A pandas DataFrame containing centroid data.
@@ -864,15 +857,14 @@
         return {}
     else:
         iso_info_s1 = (iso_info_s / iso_info_s.values.max()).sort_index().round(3)
         iso_info_s2 = iso_info_s1[iso_info_s1 > 0.015].to_dict()
         return iso_info_s2
 
 
-
 def isotope_score(iso_info, formula, mode='pos', i_threshold=2, error=0.015):
     """
     Calculate the isotope matching score based on the observed isotope distribution and 
     the theoretical distribution for a given molecular formula.
 
     Args:
         iso_info (dict): The observed isotope distribution.
@@ -902,18 +894,18 @@
         for k, v in iso_info.items():
             if (k < isotope + error) & (k > isotope - error):
                 target_distribution[i] = v
                 break
 
     # Calculate the score
     diff = target_distribution - distribution_normalized
-    score = 1 - np.sum(np.abs(diff))/sum(distribution_normalized)
+    score = 1 - np.sum(np.abs(diff)) / sum(distribution_normalized)
     return score
-    
-    
+
+
 # Reminder：I have change evaluate_ms3 to evaluate_ms3, so other place should be changed accordingly
 def evaluate_ms(new_spec, mz_exp):
     """
     Evaluate the target mass spectrum and calculate its observed m/z, error, optimal m/z, error, and resolution.
 
     Args:
         new_spec (pd.Series): The target mass spectrum, must be a profile data.
@@ -1044,15 +1036,15 @@
     x_new = np.linspace(x.min(), x.max(), num=num_points, endpoint=True)
     y_new = f(x_new)
     return x_new, y_new
 
 
 def split_peak_picking(ms1, profile=True, split_n=20, threshold=15, i_threshold=500, peak_width=2,
                        SN_threshold=3, noise_threshold=0, rt_error_alignment=0.05,
-                       mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False, orbi=False, message = ''):
+                       mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False, orbi=False, message=''):
     """
     Find peaks in the orginal ms1 list, analyze isotope and adduct information, and return a dataframe with
     information on the peaks including retention time, m/z value, intensity, and area.
 
     Args:
         ms1 (scan list): generated from sep_scans(file.mzML).
         profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
@@ -1084,79 +1076,83 @@
         new_spec[target_mz + width] = 0
         new_spec = new_spec.sort_index()
         return new_spec
 
     # Loading data...
 
     if profile is True:
-        peaks_index = [[i, scipy.signal.find_peaks(ms1[i].i.copy())[0]] 
-                       for i in tqdm(range(len(ms1)),desc = f'{message}Loading Data',leave = False,colour = 'Green')]
+        peaks_index = [[i, scipy.signal.find_peaks(ms1[i].i.copy())[0]]
+                       for i in tqdm(range(len(ms1)), desc=f'{message}Loading Data', leave=False, colour='Green')]
         raw_info_centroid = {
             round(ms1[i].scan_time[0], 3): pd.Series(
-                data=ms1[i].i[peaks], 
+                data=ms1[i].i[peaks],
                 index=ms1[i].mz[peaks].round(4),
-                name=round(ms1[i].scan_time[0], 3)) for i, peaks in tqdm(peaks_index,desc = f'{message}Convert to Centroid', leave = False,colour = 'Green')}
+                name=round(ms1[i].scan_time[0], 3)) for i, peaks in
+            tqdm(peaks_index, desc=f'{message}Convert to Centroid', leave=False, colour='Green')}
         raw_info_profile = {round(ms1[i].scan_time[0], 3):
                                 pd.Series(data=ms1[i].i, index=ms1[i].mz.round(4), name=round(ms1[i].scan_time[0], 3))
-                            for i in tqdm(range(len(ms1)),desc = f'{message}Recording raw profile info',leave = False,colour = 'Green')}
+                            for i in tqdm(range(len(ms1)), desc=f'{message}Recording raw profile info', leave=False,
+                                          colour='Green')}
         if orbi is True:
             data = []
-            for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking orbitrap data', leave = False,colour = 'Green'):
+            for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking orbitrap data', leave=False,
+                             colour='Green'):
                 df = v.to_frame().reset_index()
                 df = df.sort_values(['index', k])
                 df['index'] = np.round(df['index'].values, 3)
                 df = df.drop_duplicates('index', keep='last')
                 s = pd.Series(df[k].values, index=df['index'], name=k)
                 data.append(s)
         else:
             data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
-                    tqdm(raw_info_centroid.items(),leave = False, desc = f'{message}Appending centroid data',colour = 'Green')]
+                    tqdm(raw_info_centroid.items(), leave=False, desc=f'{message}Appending centroid data',
+                         colour='Green')]
     else:
         raw_info_centroid = {round(ms1[i].scan_time[0], 3): pd.Series(
             data=ms1[i].i, index=ms1[i].mz.round(4), name=round(ms1[i].scan_time[0], 3)) for i in
-            tqdm(range(len(ms1)),desc = f'{message}Loading Data',leave = False,colour = 'Green')}
+            tqdm(range(len(ms1)), desc=f'{message}Loading Data', leave=False, colour='Green')}
         if orbi is True:
             data = []
-            for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking orbitrap data',leave = False,colour = 'Green'):
+            for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking orbitrap data', leave=False,
+                             colour='Green'):
                 df = v.to_frame().reset_index()
                 df = df.sort_values(['index', k])
                 df['index'] = np.round(df['index'].values, 3)
                 df = df.drop_duplicates('index', keep='last')
                 s = pd.Series(df[k].values, index=df['index'], name=k)
                 data.append(s)
         else:
             data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
-                    tqdm(raw_info_centroid.items(),desc = f'{message}Appending centroid data',leave = False, colour = 'Green' )]
-
-
+                    tqdm(raw_info_centroid.items(), desc=f'{message}Appending centroid data', leave=False,
+                         colour='Green')]
 
     # 开始分割
     # 定义变量名称
     all_data = []
     for j in range(split_n):
         name = 'a' + str(j + 1)
         locals()[name] = []
     # 对series进行切割
     ms_increase = int(1000 / split_n)
-    for i in tqdm(range(len(data)), desc=f'{message}Split series', leave = False, colour = 'Green'):
+    for i in tqdm(range(len(data)), desc=f'{message}Split series', leave=False, colour='Green'):
         s1 = data[i]
         low, high = 50, 50 + ms_increase
         for j in range(split_n):
             name = 'a' + str(j + 1)
             locals()[name].append(
                 s1[(s1.index < high + mz_overlap) & (s1.index >= low - mz_overlap) & (s1.index > noise_threshold)])
             low += ms_increase
             high += ms_increase
     for j in range(split_n):
         name = 'a' + str(j + 1)
         all_data.append(locals()[name])
 
     # 开始分段提取
     all_peak_all = []
-    for data in tqdm(all_data, desc=f'{message}Split peak picking process',leave = False, colour = 'Green'):
+    for data in tqdm(all_data, desc=f'{message}Split peak picking process', leave=False, colour='Green'):
         df1 = pd.concat(data, axis=1)
         df1 = df1.fillna(0)
         if len(df1) == 0:
             pass
         else:
             peak_all = peak_picking(df1, isotope_analysis=False, threshold=threshold, peak_width=peak_width,
                                     i_threshold=i_threshold, SN_threshold=SN_threshold,
@@ -1170,60 +1166,58 @@
 
     peak_p = np.array([peak_all.rt.values, peak_all.mz.values]).T
     indice = [
         peak_all[
             (peak_all.mz > peak_p[i][1] - mz_error_alignment) & (peak_all.mz < peak_p[i][1] + mz_error_alignment) &
             (peak_all.rt > peak_p[i][0] - rt_error_alignment) & (
                     peak_all.rt < peak_p[i][0] + rt_error_alignment)].sort_values(by='intensity').index[-1] for
-        i in tqdm(range(len(peak_p)),desc = f'{message}Single file alignment',leave = False, colour = 'Green')]
+        i in tqdm(range(len(peak_p)), desc=f'{message}Single file alignment', leave=False, colour='Green')]
     indice1 = np.array(list(set(indice)))
     peak_all = peak_all.loc[indice1, :].sort_values(by='intensity', ascending=False).reset_index(drop=True)
 
-
     # 对同位素丰度进行记录
 
     raw_info_rts = [v.name for k, v in raw_info_centroid.items()]
     rts = peak_all.rt.values
     mzs = peak_all.mz.values
     rt_keys = [raw_info_rts[argmin(abs(np.array(raw_info_rts) - i))] for i in rts]  # 基于上述rt找到ms的时间索引
 
-    iso_info = [str(isotope_distribution(raw_info_centroid[rt_keys[i]], mzs[i])) 
-                for i in tqdm(range(len(mzs)),desc = f'{message}Recording iso_info',leave = False, colour = 'Green')]
+    iso_info = [str(isotope_distribution(raw_info_centroid[rt_keys[i]], mzs[i]))
+                for i in tqdm(range(len(mzs)), desc=f'{message}Recording iso_info', leave=False, colour='Green')]
     peak_all['iso_distribution'] = iso_info
 
-
     # 更新质量数据
-   
+
     if profile is True:
         spec1 = [raw_info_profile[i] for i in rt_keys]  # 获得ms的spec
         mz_result = np.array(
-            [list(evaluate_ms(target_spec1(spec1[i], mzs[i], width=0.04).copy(), mzs[i])) for i 
-             in tqdm(range(len(mzs)),desc = f'{message} Correcting m/z', leave = False)]).T
+            [list(evaluate_ms(target_spec1(spec1[i], mzs[i], width=0.04).copy(), mzs[i])) for i
+             in tqdm(range(len(mzs)), desc=f'{message} Correcting m/z', leave=False)]).T
         mz_obs, mz_opt, resolution = mz_result[0], mz_result[2], mz_result[4]
-        mz_opt = [mz_opt[i] if abs(mzs[i] - mz_opt[i]) < 0.02 else 
-                  mzs[i] for i in tqdm(range(len(mzs)),desc = f'{message}Checking the corrected m/z',leave = False, colour = 'Green')]  # 去掉偏差大的矫正结果
+        mz_opt = [mz_opt[i] if abs(mzs[i] - mz_opt[i]) < 0.02 else
+                  mzs[i] for i in tqdm(range(len(mzs)), desc=f'{message}Checking the corrected m/z', leave=False,
+                                       colour='Green')]  # 去掉偏差大的矫正结果
 
         peak_all.loc[:, ['mz', 'mz_opt', 'resolution']] = np.array([mz_obs, mz_opt, resolution.astype(int)]).T
     else:
         spec1 = [raw_info_centroid[i] for i in rt_keys]  # 获得ms的spec
         target_spec = [spec1[i][(spec1[i].index > mzs[i] - 0.015) & (spec1[i].index < mzs[i] + 0.015)] for i in
-                       tqdm(range(len(spec1)),desc = f'{message}Correcting m/z',leave = False, colour = 'Green')]
+                       tqdm(range(len(spec1)), desc=f'{message}Correcting m/z', leave=False, colour='Green')]
         mzs_obs = [
             target_spec[i].index.values[[np.argmax(target_spec[i].values)]][0] if len(target_spec[i]) != 0 else mzs[i]
-            for i in tqdm(range(len(target_spec)),desc = f'{message}Checking obs m/z',leave = False, colour = 'Green')]
+            for i in tqdm(range(len(target_spec)), desc=f'{message}Checking obs m/z', leave=False, colour='Green')]
         peak_all['mz'] = mzs_obs
 
-
-
     # 如果担心饱和质量不准，使用sat_intensity 更新质量
     if (sat_intensity is False) | (sat_intensity is None):
         pass
     else:
         if profile is True:
-            for j in tqdm(range(len(peak_all)), desc=f'{message}Optimize m/z based on sat_intensity',leave = False, colour = 'Green'):
+            for j in tqdm(range(len(peak_all)), desc=f'{message}Optimize m/z based on sat_intensity', leave=False,
+                          colour='Green'):
                 mz = peak_all.loc[j, 'mz']
                 rt = peak_all.loc[j, 'rt']
                 intensity = peak_all.loc[j, 'intensity']
                 if intensity > sat_intensity:
                     for k, v in raw_info_profile.items():
                         if k > rt:  # find the time
                             s1 = raw_info_profile[k]
@@ -1233,15 +1227,16 @@
                             if max(new_spec1.values[peak_index]) < sat_intensity:
                                 insat_mz_obs, error1, insat_mz_opt, error2, resolution = evaluate_ms(new_spec1, mz)
                                 peak_all.loc[j, 'mz'] = insat_mz_obs
                                 peak_all.loc[j, 'mz_opt'] = insat_mz_opt
                                 peak_all.loc[j, 'resolution'] = resolution
                                 break
         else:
-            for j in tqdm(range(len(peak_all)), desc=f'{message}Optimize m/z based on sat_intensity',leave = False, colour = 'Green'):
+            for j in tqdm(range(len(peak_all)), desc=f'{message}Optimize m/z based on sat_intensity', leave=False,
+                          colour='Green'):
                 mz = peak_all.loc[j, 'mz']
                 rt = peak_all.loc[j, 'rt']
                 intensity = peak_all.loc[j, 'intensity']
                 if intensity > sat_intensity:
                     for k, v in raw_info_centroid.items():
                         if k > rt:  # find the time
                             s1 = raw_info_centroid[k]
@@ -1294,15 +1289,15 @@
     Ciso = atom_mass_table1['Ciso'] - atom_mass_table1['C']
     Cliso = atom_mass_table1['Cliso'] - atom_mass_table1['Cl']
     Na = atom_mass_table1['Na'] - atom_mass_table1['H']
     K = atom_mass_table1['K'] - atom_mass_table1['H']
     NH3 = 3 * atom_mass_table1['H'] + atom_mass_table1['N']
 
     all_rts = list(set(cmp['rt'].values))
-    for i in tqdm(range(len(all_rts)), desc='Finding Isotopes and adducts',leave = False):
+    for i in tqdm(range(len(all_rts)), desc='Finding Isotopes and adducts', leave=False):
         cmp_rt = cmp[(cmp['rt'] >= all_rts[i] - 0.015) & (cmp['rt'] <= all_rts[i] + 0.015)].sort_values(by='mz')
         mzs = cmp_rt['mz'].values
         for mz in mzs:
             C_fold = 1
             differ = mzs - mz
             # 拿到此mz的intensity
             mz_i = cmp_rt[cmp_rt['mz'] == mz]['intensity'].values[0]  # 数值
@@ -1423,19 +1418,18 @@
         files_excel: files path list for excels of peak picking and peak checking;
         rt_error: rt error for combining
         mz_error: mz error for combining
     returns:
         Export to excel files
     """
 
-    
     peak_ref = gen_ref(files_excel, rt_error=rt_error, mz_error=mz_error)
     pd.DataFrame(peak_ref, columns=['rt', 'mz']).to_excel(
         os.path.join(os.path.split(files_excel[0])[0], 'peak_ref.xlsx'))
-    for file in tqdm(files_excel, desc='Alignment',leave = False, colour = 'Green'):
+    for file in tqdm(files_excel, desc='Alignment', leave=False, colour='Green'):
         peak_p = pd.read_excel(file, index_col='Unnamed: 0').loc[:, ['rt', 'mz']].values
         peak_df = pd.read_excel(file, index_col='Unnamed: 0')
         new_all_index = []
         for i in range(len(peak_p)):
             rt1, mz1 = peak_p[i]
             index = np.where((peak_ref[:, 0] <= rt1 + rt_error) & (peak_ref[:, 0] >= rt1 - rt_error)
                              & (peak_ref[:, 1] <= mz1 + mz_error) & (peak_ref[:, 1] >= mz1 - mz_error))
@@ -1444,21 +1438,21 @@
         peak_df['new_index'] = new_all_index
         peak_df = peak_df.set_index('new_index')
         peak_df = peak_df[~peak_df.index.duplicated(keep='first')]
         peak_df.to_excel(file.replace('.xlsx', '_alignment.xlsx'))
 
 
 def gen_ref(files_excel, rt_error=0.1, mz_error=0.015):
-    data1 = [pd.read_excel(file).loc[:, ['rt', 'mz']].values 
+    data1 = [pd.read_excel(file).loc[:, ['rt', 'mz']].values
              for file in tqdm(files_excel, desc='Reading each excel file',
-                              leave = False,colour = 'Green',ncols = 100)]
+                              leave=False, colour='Green', ncols=100)]
 
     # Concatenate all peaks
     data = np.vstack(data1)
-    
+
     # Scale the tolerances relative to the range of each dimension
     rt_range = np.ptp(data[:, 0])
     mz_range = np.ptp(data[:, 1])
     scaled_rt_tol = rt_error / rt_range
     scaled_mz_tol = mz_error / mz_range
 
     # Create a KDTree with scaled data
@@ -1466,57 +1460,56 @@
     scaled_data[:, 0] /= rt_range
     scaled_data[:, 1] /= mz_range
     tree = KDTree(scaled_data)
 
     reference_list = []
     visited = set()
 
-    for idx, scaled_pair in tqdm(enumerate(scaled_data),desc = 'Aligning all rt_mz pairs',leave = False, colour = 'Green'):
+    for idx, scaled_pair in tqdm(enumerate(scaled_data), desc='Aligning all rt_mz pairs', leave=False, colour='Green'):
         if idx in visited:
             continue
 
         # Find neighbors within a spherical range that's sure to encompass the rectangular range
         neighbors = tree.query_ball_point(scaled_pair, r=max(scaled_rt_tol, scaled_mz_tol))
 
         # Filter these neighbors based on the actual tolerances
-        filtered_neighbors = [i for i in neighbors if abs(data[i, 0] - data[idx, 0]) <= rt_error 
+        filtered_neighbors = [i for i in neighbors if abs(data[i, 0] - data[idx, 0]) <= rt_error
                               and abs(data[i, 1] - data[idx, 1]) <= mz_error]
 
         # Mark these neighbors as visited and add the first one to the reference list
         visited.update(filtered_neighbors)
         reference_list.append(data[idx])
 
     return np.array(reference_list)
 
 
-def second_process(file, ref_all, company, profile=True, long_rt_split_n=1, orbi=False, message = ''):
+def second_process(file, ref_all, company, profile=True, long_rt_split_n=1, orbi=False, message=''):
     """
     This function will use the reference rt&mz pair, and obtain the peak area at specific rt & mz
     Args:
         profile: True or False
         file: single file to process
         ref_all: all reference peaks
         company: e.g., 'Waters', 'Agilent',etc,
         orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False)
     returns:
         export to files
 
     """
     ms_round = 4
-    ms1, ms2 = sep_scans(file, company, message = message)
+    ms1, ms2 = sep_scans(file, company, message=message)
 
     name1 = os.path.basename(file).split('.')[0]
     final_result = ultimate_checking_area(ref_all, ms1, name1, profile=profile,
-                                          rt_overlap=1, long_rt_split_n=long_rt_split_n, orbi=orbi,message = message)
+                                          rt_overlap=1, long_rt_split_n=long_rt_split_n, orbi=orbi, message=message)
     final_result.to_excel(file.replace('.mzML', '_final_area.xlsx'))
 
 
-
 def ultimate_checking_area(ref_all, ms1, name1, profile=True,
-                           split_n=20, rt_overlap=1, long_rt_split_n=4, orbi=False, message = ''):
+                           split_n=20, rt_overlap=1, long_rt_split_n=4, orbi=False, message=''):
     """
     Based on peak reference, intergrate peak are for each reference m/z and retention time pair.
 
     Args:
         ref_all: reference m/z and retention time pair
         ms1: generated from sep_scans(file.mzML).
         name1: file name.
@@ -1527,15 +1520,15 @@
         orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False)
     return:
         The final areas for reference m/z and retention time pair.
     """
 
     if long_rt_split_n == 1:
         final_area = peak_checking_area_split(ref_all, ms1, name1, profile=profile, split_n=split_n, noise_threshold=0,
-                                              orbi=orbi,message = message)
+                                              orbi=orbi, message=message)
 
     else:
         # Calculate the length of each part
         total_spectra = len(ms1)
         part_length = total_spectra // long_rt_split_n
         overlap_spectra = int(rt_overlap / (ms1[1].scan_time[0] - ms1[0].scan_time[
             0]))  # calculate the number of spectra in 1 minute of retention time
@@ -1568,115 +1561,122 @@
             each_peak_area = peak_checking_area_split(ref_all_parts[i], parts[i], '', profile=profile, split_n=split_n,
                                                       orbi=orbi)
             peak_area_all.append(each_peak_area)
         final_area = pd.concat(peak_area_all)
         final_area.columns = [name1]
     return final_area
 
-def peak_checking_area(ref_all, df1, name, sn_info = False):
+
+def peak_checking_area(ref_all, df1, name, sn_info=False):
     """
     Obtain the area for each rt&mz pair in df1
     :param ref_all:  peak_reference
     :param df1: dataframe df1
     :param name: name
     :return: new_dataframe
     """
 
     # 1. sort ref_all, obtain the rts and mzs
     ref_all1 = ref_all.sort_values(by='mz')
     rts, mzs = ref_all1.rt.values, ref_all1.mz.values
-    peak_index = np.array([str(rts[i])+'_'+str(mzs[i]) for i in range(len(rts))])
+    peak_index = np.array([str(rts[i]) + '_' + str(mzs[i]) for i in range(len(rts))])
     # 2. find locators of mz
     df_mz_list = df1.index.values
     left_locator = find_locators(df_mz_list, mzs - 0.01)
     right_locator = find_locators(df_mz_list, mzs + 0.01)
     mz_locators = np.array([left_locator, right_locator]).T
 
     # 3. find the locators of rt
     df_rt = df1.columns.values
     rt_locators = [[argmin(abs(df_rt - (rt - 0.2))), argmin(abs(df_rt - (rt + 0.2)))] for rt in rts]
     rt_locators = [[x[0], x[1] if x[0] != x[1] else x[1] + 1] for x in rt_locators]  # 有时候locators是一样的[616:616] 加个保护机制
     # 4. obtain the peak areas
     area_all = [round(scipy.integrate.simps(
         df1.iloc[mz_locators[i][0]:mz_locators[i][1], rt_locators[i][0]:rt_locators[i][1]].values.sum(axis=0) - min(
-            df1.iloc[mz_locators[i][0]:mz_locators[i][1], rt_locators[i][0]:rt_locators[i][1]].values.sum(axis=0))), 
-                      0)+1 for i in range(len(mz_locators))]
+            df1.iloc[mz_locators[i][0]:mz_locators[i][1], rt_locators[i][0]:rt_locators[i][1]].values.sum(axis=0))),
+        0) + 1 for i in range(len(mz_locators))]
     # Uses the locators found in stepas 2 and 3 to calculate the peak areas for each rt&mz pair in `df1`, using the `scipy.integrate.simps` function.
 
     if sn_info == False:
         sample_area = pd.DataFrame(area_all, index=peak_index, columns=[name])
         return sample_area  # Adds 1 to each value in the resulting data frame and returns it to avoid zero value in result.
     else:
         rt_locators_point = [argmin(abs(df_rt - rt)) for rt in rts]
         sn_all = []
         for i in range(len(mz_locators)):
-            df2 = df1.iloc[mz_locators[i][0]:mz_locators[i][1],:]
+            df2 = df1.iloc[mz_locators[i][0]:mz_locators[i][1], :]
             eic = df2.values.sum(axis=0)
             try:
                 bg = int(cal_bg(eic))
             except:
                 bg = np.inf
             peak_height = eic[rt_locators_point[i]]
-            sn = round(peak_height/bg,1)
+            sn = round(peak_height / bg, 1)
             sn_all.append(sn)
-        sample_area = pd.DataFrame({f'{name}':area_all,f'{name}_S/N':sn_all}, index=peak_index)
+        sample_area = pd.DataFrame({f'{name}': area_all, f'{name}_S/N': sn_all}, index=peak_index)
         return sample_area
 
-def peak_checking_area_split(ref_all, ms1, name1, profile=True, split_n=20, noise_threshold=0, orbi=False,message = '',sn_info = False):
+
+def peak_checking_area_split(ref_all, ms1, name1, profile=True, split_n=20, noise_threshold=0, orbi=False, message='',
+                             sn_info=False):
     # 需要给ref_all排序
-    
+
     ref_all1 = ref_all.sort_values(by='mz')
 
     if profile is True:
-        peaks_index = [[i, scipy.signal.find_peaks(ms1[i].i.copy())[0]] 
-                       for i in tqdm(range(len(ms1)),desc = f'{message}Loading Data',leave = False,colour = 'Green')]
+        peaks_index = [[i, scipy.signal.find_peaks(ms1[i].i.copy())[0]]
+                       for i in tqdm(range(len(ms1)), desc=f'{message}Loading Data', leave=False, colour='Green')]
         raw_info_centroid = {
             round(ms1[i].scan_time[0], 3): pd.Series(
                 data=ms1[i].i[peaks], index=ms1[i].mz[peaks].round(4),
                 name=round(ms1[i].scan_time[0], 3)) for i, peaks in tqdm(
-                peaks_index,desc = f'{message}Loading Data',leave = False,colour = 'Green')}
+                peaks_index, desc=f'{message}Loading Data', leave=False, colour='Green')}
         if orbi is True:
             data = []
-            for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking orbitrap data',leave = False,colour = 'Green'):
+            for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking orbitrap data', leave=False,
+                             colour='Green'):
                 df = v.to_frame().reset_index()
                 df = df.sort_values(['index', k])
                 df['index'] = np.round(df['index'].values, 3)
                 df = df.drop_duplicates('index', keep='last')
                 s = pd.Series(df[k].values, index=df['index'], name=k)
                 data.append(s)
         else:
             data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
-                    tqdm(raw_info_centroid.items(), desc = f'{message}Appending centroid data',leave = False,colour = 'Green')]
+                    tqdm(raw_info_centroid.items(), desc=f'{message}Appending centroid data', leave=False,
+                         colour='Green')]
     else:
         raw_info_centroid = {round(ms1[i].scan_time[0], 3): pd.Series(
             data=ms1[i].i, index=ms1[i].mz.round(4), name=round(ms1[i].scan_time[0], 3)) for i in
-            tqdm(range(len(ms1)),desc = f'{message}Loading Data',leave = False,colour = 'Green')}
+            tqdm(range(len(ms1)), desc=f'{message}Loading Data', leave=False, colour='Green')}
 
         if orbi is True:
             data = []
-            for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking orbitrap data',leave = False,colour = 'Green'):
+            for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking orbitrap data', leave=False,
+                             colour='Green'):
                 df = v.to_frame().reset_index()
                 df = df.sort_values(['index', k])
                 df['index'] = np.round(df['index'].values, 3)
                 df = df.drop_duplicates('index', keep='last')
                 s = pd.Series(df[k].values, index=df['index'], name=k)
                 data.append(s)
         else:
             data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
-                    tqdm(raw_info_centroid.items(), desc = f'{message}Appending centroid data',leave = False, colour = 'Green' )]
+                    tqdm(raw_info_centroid.items(), desc=f'{message}Appending centroid data', leave=False,
+                         colour='Green')]
 
     # 开始分割 series数据
     # 定义变量名称
     all_data = []
     for j in range(split_n):
         name = 'a' + str(j + 1)
         locals()[name] = []
     # 对series进行切割
     ms_increase = int(1000 / split_n)
-    for i in tqdm(range(len(data)), desc=f'{message}Split series', leave = False, colour = 'Green'):
+    for i in tqdm(range(len(data)), desc=f'{message}Split series', leave=False, colour='Green'):
         s1 = data[i]
         low, high = 50, 50 + ms_increase
         for j in range(split_n):
             name = 'a' + str(j + 1)
             locals()[name].append(s1[(s1.index < high + 0.1) & (s1.index >= low - 0.1) & (s1.index > noise_threshold)])
             low += ms_increase
             high += ms_increase
@@ -1694,27 +1694,27 @@
         locals()[name] = ref_all1[(ref_all1.mz < high) & (ref_all1.mz >= low)]
         low += ms_increase
         high += ms_increase
         all_peak_ref.append(locals()[name])
 
     # 获取所有area
     area_all = []
-    for i in tqdm(range(split_n), desc=f'{message}Collecting area info',leave = False, colour = 'Green'):
+    for i in tqdm(range(split_n), desc=f'{message}Collecting area info', leave=False, colour='Green'):
         peak_ref1 = all_peak_ref[i]
         df1 = pd.concat(all_data[i], axis=1)
         if len(df1) == 0:
             pass
         else:
             df1 = df1.fillna(0).sort_index()
-            df_area = peak_checking_area(peak_ref1, df1, 'split',sn_info = sn_info)
+            df_area = peak_checking_area(peak_ref1, df1, 'split', sn_info=sn_info)
             area_all.append(df_area)
 
     # 合成所有的area
     final_df = pd.concat(area_all)
-    final_df.columns = [name1] if len(final_df.columns) ==1 else [name1,f'{name1}_S/N']
+    final_df.columns = [name1] if len(final_df.columns) == 1 else [name1, f'{name1}_S/N']
     return final_df
 
 
 def concat_alignment(files_excel):
     """
     Concatenate all dataframes containing 'area' in their name
     and return the final dataframe.
@@ -1724,15 +1724,15 @@
 
     Returns:
         pandas.DataFrame: concatenated dataframe.
     """
     align = [file for file in files_excel if 'area' in file]
     data = {}
     data_to_concat = []
-    for i in tqdm(range(len(align)), desc='Concatenating all areas',leave = False, colour = 'Green'):
+    for i in tqdm(range(len(align)), desc='Concatenating all areas', leave=False, colour='Green'):
         name = 'data' + str(i)
         data[name] = pd.read_excel(align[i], index_col='Unnamed: 0')
         data_to_concat.append(data[name])
     final_data = pd.concat(data_to_concat, axis=1)
     return final_data
 
 
@@ -1781,15 +1781,15 @@
     # 整合每个area_file与blank的对比结果，输出fold change 大于fold_change倍的值
     area_files_sample = [file for file in area_files if
                          not any(group.lower() in os.path.basename(file).lower() for group in control_group)]
     all_names = list(
         set([os.path.basename(x).replace('_final_area.xlsx', '')[:-1] for x in area_files_sample]))  # 拿到所有样品名称
 
     if filter_type == 1:
-        for i in tqdm(range(len(area_files_sample)), desc='Fold change processing', leave = False, colour = 'Green'):
+        for i in tqdm(range(len(area_files_sample)), desc='Fold change processing', leave=False, colour='Green'):
             # 基于峰面积的对比拿到比较数据
             sample = pd.read_excel(area_files_sample[i], index_col='Unnamed: 0')
             # 开始处理alignment文件，不能有重复的index
             name = os.path.basename(area_files_sample[i]).replace('_final_area.xlsx', '')  # 拿到名字
             alignment_path = [file for file in alignment if name in file][0]
             alignment_df = pd.read_excel(alignment_path, index_col='new_index').sort_values(by='intensity')
             alignment_df1 = alignment_df[~alignment_df.index.duplicated(keep='last')]  # 去掉重复索引
@@ -1808,15 +1808,15 @@
                 compare2 = compare1.iloc[:, -1:]
                 alignment_df1 = pd.concat([alignment_df1, compare2], axis=1)
             alignment_df1 = alignment_df1.sort_values(by='intensity', ascending=False)
             alignment_df1.index.name = 'new_index'
             alignment_df1.to_excel(alignment_path.replace('_alignment', '_unique_cmps'))
     elif filter_type == 2:
         # 根据样品名称一个个处理
-        for name in tqdm(all_names, desc='Processing triplicate samples', leave = False, colour = 'Green'):
+        for name in tqdm(all_names, desc='Processing triplicate samples', leave=False, colour='Green'):
             # 获得该名称下的文件
             sample_files = [file for file in area_files_sample if name in os.path.basename(file)]
             # 获得所有final_area
             sample_df_all = []
             for sample_file in sample_files:
                 df = pd.read_excel(sample_file, index_col='Unnamed: 0')
                 sample_df_all.append(df)
@@ -1874,40 +1874,47 @@
 
     # Extract peak index and value data
     peak_index = profile_data.index.values[peaks]
     peak_values = profile_data.values[peaks]
 
     # Create new Series with peak data as centroid data
     if len(peak_index) > 0:
-        centroid_data = pd.Series(peak_values, peak_index, name=profile_data.name,dtype='float64')
+        centroid_data = pd.Series(peak_values, peak_index, name=profile_data.name, dtype='float64')
     else:
-        centroid_data = pd.Series(name=profile_data.name,dtype='float64')
+        centroid_data = pd.Series(name=profile_data.name, dtype='float64')
 
     return centroid_data
 
-def gen_DDA_ms2_df(ms1,ms2, i_threshold=0, profile=True, opt=False, more_info = False,message = ''):
+
+def gen_DDA_ms2_df(ms1, ms2, i_threshold=0, profile=True, opt=False, more_info=False, message=''):
     """
-    Reads DDA data and generates a DataFrame with rt, precursor and fragments info.
+    Generates a DataFrame from DDA MS2 data with detailed information on retention times, 
+    precursors, fragments, and additional metrics depending on specified options.
 
     Args:
-        path (str): Path to single file
-        company (str): The type of mass spectrometer used to acquire the data. 
-                       Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
-        i_threshold (float): Intensity threshold for filtering out low intensity peaks. Default is 0.
-        profile (bool): Boolean value that indicates whether the data is in profile or centroid mode. 
-                        True for profile mode, False for centroid mode. Default is True.
-        opt (bool): Boolean value indicating whether to optimize the mass. Default is True.
+        ms1 (list): List of MS1 scans.
+        ms2 (list): List of MS2 scans.
+        i_threshold (float): Minimum intensity threshold for peak consideration. Defaults to 0.
+        profile (bool): Indicates if data is in profile mode (True) or centroid mode (False). Defaults to True.
+        opt (bool): If True, performs optimization on mass data. Defaults to False.
+        more_info (bool): If True, additional information from MS1 is appended to the DataFrame. Defaults to False.
+        message (str): Message to display during extended information gathering. Defaults to an empty string.
 
     Returns:
-        DataFrame: DataFrame with rt, precursor and fragments info
-    """
+        pandas.DataFrame: Contains columns for retention time (rt), precursor m/z, fragments (frag), 
+                          intensity, collision energy, mode of ionization, scan index, MS2 spectra,
+                          isotope distribution, and optionally optimized fragment m/z and MS1 data.
 
+    Note:
+        The function supports dynamic generation of data based on the `profile`, `opt`, and `more_info` flags,
+        adapting the output DataFrame accordingly. The function requires tqdm and pandas libraries for execution.
+    """
 
     # Initialize empty lists to hold precursor, rt, fragment, intensity, collision energy, and mode data
-    precursors, rts, frags, intensities, collision_energies, modes, scan_indices, s_all = [], [], [], [], [], [], [],[]
+    precursors, rts, frags, intensities, collision_energies, modes, scan_indices, s_all, iso_info = [], [], [], [], [], [], [], [], []
 
     # Loop through each MS2 scan
     for i, scan in tqdm(enumerate(ms2), desc='Collecting MS2 info', colour='Green', leave=False):
         # Get the precursor m/z value
         precursor = scan.selected_precursors[0]['mz']
         precursors.append(precursor)
 
@@ -1930,75 +1937,83 @@
         else:
             modes.append('Unknown')
 
         # Get the m/z and intensity values from the scan
         mz = scan.mz
         intensity = scan.i
 
+        # Get the isotope information
+        spec = pd.Series(index=np.round(mz, 4), data=intensity, name=rt)
+        iso_result = isotope_distribution(spec, precursor, error=0.02)
+        iso_info.append(iso_result)
+
         # If data is in profile mode, convert it to centroid mode
         if profile is True:
             spec = pd.Series(data=intensity, index=mz, dtype='float64')  # Explicit dtype
             new_spec = ms_to_centroid(spec)
             mz = new_spec.index.values
             intensity = new_spec.values
 
         # Filter out low intensity peaks and get the top 20 peaks
-        s = pd.Series(data=intensity, index=mz, dtype='float64').sort_values(ascending=False).iloc[:20]  # Explicit dtype
+        s = pd.Series(data=intensity, index=np.round(mz, 4), dtype='float64').sort_values(ascending=False).iloc[
+            :20]  # Explicit dtype
         s = s[s > i_threshold]
 
         # Get the fragment m/z values and their intensities
         s_all.append(str(s))
         frag = list(s.index.values.round(4))
         frags.append(frag)
         intensities.append(list(s.values))
-        
+
     # Create a DataFrame with precursor, rt, fragment, intensity, collision energy, mode, and scan index data
-    DDA_df = pd.DataFrame([precursors, rts, collision_energies, frags, intensities, modes, scan_indices,s_all],
-                          index=['precursor', 'rt', 'collision energy', 'frag', 
-                                 'intensity', 'mode', 'scan_index','MS2_spectra']).T
+    DDA_df = pd.DataFrame(
+        [precursors, rts, collision_energies, frags, intensities, modes, scan_indices, s_all, iso_info],
+        index=['precursor', 'rt', 'collision energy', 'frag',
+               'intensity', 'mode', 'scan_index', 'MS2_spectra', 'iso_distribution']).T
 
     # Optimize mass if required
     if profile and opt:
         mz_opt_all = []
         for i in tqdm(range(len(DDA_df)), desc='Optimizing mass'):
             frag = DDA_df.loc[i].frag
             x = DDA_df.loc[i].scan_index
             s1 = pd.Series(ms2[x].i, ms2[x].mz.round(4), dtype='float64')  # Explicit dtype
             mz_opt = [evaluate_ms(target_spec(s1, mz), mz)[2] for mz in frag]
             mz_opt_all.append(mz_opt)
 
         DDA_df['frag_opt'] = mz_opt_all
-    
+
     # find the intensity
     if more_info is True:
-        for i in tqdm(range(len(DDA_df)),desc = f'{message}Obtain more information',colour = 'Green',leave = False):
-            rt = DDA_df.loc[i,'rt']
-            precursor = DDA_df.loc[i,'precursor']
+        for i in tqdm(range(len(DDA_df)), desc=f'{message}Obtain more information', colour='Green', leave=False):
+            rt = DDA_df.loc[i, 'rt']
+            precursor = DDA_df.loc[i, 'precursor']
             for j in range(len(ms1)):
-                if (ms1[j].scan_time[0]>=rt)&(j>1):
-                    spec = pd.Series(data = ms1[j-1].i,index = ms1[j-1].mz)
+                if (ms1[j].scan_time[0] >= rt) & (j > 1):
+                    spec = pd.Series(data=ms1[j - 1].i, index=ms1[j - 1].mz)
                     if profile is True:
-                        spec1 = target_spec(spec,precursor,width=0.2)
+                        spec1 = target_spec(spec, precursor, width=0.2)
 
                         intensity = spec1.max()
-                        mz_obs,error1,mz_opt,error2, resolution = evaluate_ms(spec1,precursor)
-                        DDA_df.loc[i,'ms1_intensity']=intensity
-                        DDA_df.loc[i,'ms1_obs'] = mz_obs
+                        mz_obs, error1, mz_opt, error2, resolution = evaluate_ms(spec1, precursor)
+                        DDA_df.loc[i, 'ms1_intensity'] = intensity
+                        DDA_df.loc[i, 'ms1_obs'] = mz_obs
 
                     else:
-                        spec1 = target_spec(spec,precursor,width=0.2)
+                        spec1 = target_spec(spec, precursor, width=0.2)
                         intensity = spec1.max()
-                        ms1_obs = spec1.index[np.argmin(abs(spec1.index-precursor))]
-                        DDA_df.loc[i,'ms1_intensity']=intensity
-                        DDA_df.loc[i,'ms1_obs'] = ms1_obs 
+                        ms1_obs = spec1.index[np.argmin(abs(spec1.index - precursor))]
+                        DDA_df.loc[i, 'ms1_intensity'] = intensity
+                        DDA_df.loc[i, 'ms1_obs'] = ms1_obs
 
                     break
-                    
+
     return DDA_df
 
+
 def ms2_matching(unique, database, ms1_error=50, ms2_error=0.015, mode='pos', frag_DIA='frag_DIA', frag_DDA='frag_DDA'):
     """
     Match masses and fragments by comparing these values with those in the database.
 
     Args:
         unique (pandas.DataFrame): DataFrame containing unique compounds to be matched.
         database (pandas.DataFrame): DataFrame containing the database to match against.
@@ -2014,28 +2029,29 @@
     if 'Source' not in database.columns.values:
         database['Source'] = 'None'
     if 'Source info' not in database.columns.values:
         database['Source info'] = 'None'
 
     database['Source'] = database['Source'].fillna('None')
     database['Source info'] = database['Source info'].fillna('None')
-    
-    adduct = '+H' if mode == 'pos' else '-H' 
+
+    adduct = '+H' if mode == 'pos' else '-H'
     columns = list(unique.columns.values)
     DIA = [column for column in columns if frag_DIA in column]
     DDA = [column for column in columns if frag_DDA in column]
     print(' ')
     print('DIA columns:', DIA)
-    print('DIA columns:', DDA)
+    print('DDA columns:', DDA)
     database1 = database[database['mode'] == mode]  # 匹配mode模式
     if len(DIA) != 0:
-        for i in tqdm(range(len(unique)), desc='Starting DIA ms2 matching:',leave = False):
+        for i in tqdm(range(len(unique)), desc='Starting DIA ms2 matching:', leave=False):
             mz = unique.loc[i]['mz']
             mz_opt = unique.loc[i]['mz_opt'] if 'mz_opt' in unique.columns.values else None  # 如果有mz_opt则读入
-            iso_info = eval(unique.loc[i,'iso_distribution']) # 增加iso_info
+            iso_info = eval(
+                unique.loc[i, 'iso_distribution']) if 'iso_distribution' in unique.columns else None  # 增加iso_info
             if mode == 'pos':
                 precursor = mz - 1.0073
                 precursor_opt = mz_opt - 1.0073 if mz_opt is not None else None
             else:
                 precursor = mz + 1.0073
                 precursor_opt = mz_opt + 1.0073 if mz_opt is not None else None
             frag_obs = np.array(eval(unique.loc[i][DIA[0]]))
@@ -2055,19 +2071,19 @@
             if len(match_result) == 0:  # 匹配失败
                 pass
             else:
                 for j in range(len(match_result)):
                     ik_match = match_result['Inchikey'].iloc[j]  # 匹配的ik
                     source = match_result.iloc[j]['Source']
                     source_info = match_result.iloc[j]['Source info']
-                    
+
                     formula = match_result.iloc[j]['Formula']
-                    
+
                     try:
-                        iso_score = isotope_score(iso_info,formula,mode = mode)
+                        iso_score = isotope_score(iso_info, formula, mode=mode)
                     except:
                         iso_score = 0
                     precursor_match = match_result['Precursor'].iloc[j]
                     ms1_error_obs = round((precursor_match - precursor) / precursor * 1e6, 1)  # 计算ms1 error
                     ms1_error_opt = round((precursor_match - precursor_opt) / precursor_opt * 1e6,
                                           1) if precursor_opt is not None else None  # 计算ms1_opt error
                     try:
@@ -2094,35 +2110,36 @@
                         single_result_dict['ms1_opt_error'] = ms1_error_opt
                         single_result_dict['iso_score'] = iso_score
                         single_result_dict['match_num'] = match_num
                         single_result_dict['match_percent'] = match_percent
                         single_result_dict['match_info'] = compare_frag_dict
                         single_result_dict['Source'] = source
                         single_result_dict['Source info'] = source_info
-                        
-                        
+
                         match_result_dict.append(single_result_dict)
             # 输出结果
             unique.loc[i, 'match_result_DIA'] = str(match_result_dict)
             if len(match_result_dict) == 0:
                 unique.loc[i, 'best_results_DIA'] = str([])
             else:
 
                 optimized_result = pd.concat([pd.Series(a) for a in match_result_dict], axis=1).T
                 optimized_result['ms1_error_obs'] = optimized_result['ms1_error'].abs()
                 optimized_result = optimized_result.sort_values(
-                    by=['match_num', 'ms1_error_obs', 'iso_score','match_percent'], ascending=[False, True,False, False])
-                optimized_result.drop('ms1_error_obs',axis=1,inplace = True)
+                    by=['match_num', 'ms1_error_obs', 'iso_score', 'match_percent'],
+                    ascending=[False, True, False, False])
+                optimized_result.drop('ms1_error_obs', axis=1, inplace=True)
                 unique.loc[i, 'best_results_DIA'] = str(optimized_result.iloc[0].to_dict())
 
     if len(DDA) != 0:
-        for i in tqdm(range(len(unique)), desc='Starting DDA ms2 matching:',leave = False):
+        for i in tqdm(range(len(unique)), desc='Starting DDA ms2 matching:', leave=False):
             mz = unique.loc[i]['mz']
             mz_opt = unique.loc[i]['mz_opt'] if 'mz_opt' in unique.columns.values else None  # 如果有mz_opt则读入
-            iso_info = eval(unique.loc[i,'iso_distribution']) # 增加iso_info
+            iso_info = eval(
+                unique.loc[i, 'iso_distribution']) if 'iso_distribution' in unique.columns else None  # 增加iso_info
             if mode == 'pos':
                 precursor = mz - 1.0078
                 precursor_opt = mz_opt - 1.0073 if mz_opt is not None else None
             else:
                 precursor = mz + 1.0078
                 precursor_opt = mz_opt + 1.0073 if mz_opt is not None else None
 
@@ -2144,15 +2161,15 @@
             else:
                 for j in range(len(match_result)):
                     ik_match = match_result['Inchikey'].iloc[j]  # 匹配的ik
                     source_info = match_result.iloc[j]['Source info']
                     source = match_result.iloc[j]['Source']
                     formula = match_result.iloc[j]['Formula']
                     try:
-                        iso_score = isotope_score(iso_info,formula,mode = mode)
+                        iso_score = isotope_score(iso_info, formula, mode=mode)
                     except:
                         iso_score = 0
 
                     precursor_match = match_result['Precursor'].iloc[j]
                     ms1_error_obs = round((precursor_match - precursor) / precursor * 1e6, 1)  # 计算ms1 error
                     ms1_error_opt = round((precursor_match - precursor_opt) / precursor_opt * 1e6,
                                           1) if precursor_opt is not None else None  # 计算ms1_opt error
@@ -2186,18 +2203,19 @@
             if len(match_result_dict) == 0:
                 unique.loc[i, 'best_results_DDA'] = '[]'
             else:
 
                 optimized_result = pd.concat([pd.Series(a) for a in match_result_dict], axis=1).T
                 optimized_result['ms1_error_obs'] = optimized_result['ms1_error'].abs()
 
-                optimized_result= optimized_result.sort_values(
-                    by=['match_num', 'ms1_error_obs', 'iso_score','match_percent'], ascending=[False, True,False, False])
+                optimized_result = optimized_result.sort_values(
+                    by=['match_num', 'ms1_error_obs', 'iso_score', 'match_percent'],
+                    ascending=[False, True, False, False])
 
-                optimized_result.drop('ms1_error_obs',axis=1,inplace = True)
+                optimized_result.drop('ms1_error_obs', axis=1, inplace=True)
                 unique.loc[i, 'best_results_DDA'] = str(optimized_result.iloc[0].to_dict())
     return unique
 
 
 def compare_frag(frag_obs, frag_exp, error=0.015):
     """
     Compare the similarity of observed and expected fragments.
@@ -2361,16 +2379,16 @@
         mode (str): The ionization mode ('pos' for positive or 'neg' for negative).
 
     Returns:
         pandas.DataFrame: A result dataframe with the matching results.
     """
     adduct = '+H' if mode == 'pos' else '-H'
     db = database[(database['mode'] == mode) & (~database['rt'].isna())]
-    for i in tqdm(range(len(unique)), desc='Starting rt & m/z matching:',leave = False):
-        iso_info = eval(unique.loc[i,'iso_distribution'])
+    for i in tqdm(range(len(unique)), desc='Starting rt & m/z matching:', leave=False):
+        iso_info = eval(unique.loc[i, 'iso_distribution']) if 'iso_distribution' in unique.columns else None
         if 'mz_opt' in unique.columns.values:
             rt, mz, mz_opt = unique.loc[i, ['rt', 'mz', 'mz_opt']]
         else:
             rt, mz = unique.loc[i, ['rt', 'mz']]
             mz_opt = None
 
         if mode == 'pos':
@@ -2398,23 +2416,23 @@
             result1['rt_error'] = (result1['rt'] - rt).round(3)
             result1['mz_error'] = ((result1['Precursor'] - precursor) / precursor * 1e6).round(1)
             result1['mz_opt_error'] = ((result1['Precursor'] - mz_opt1) / mz_opt1 * 1e6).round(
                 1) if mz_opt1 is not None else None
             result1['ik'] = result1['Inchikey']
 
             try:
-                iso_score = isotope_score(iso_info,formula,mode = mode)
+                iso_score = isotope_score(iso_info, formula, mode=mode)
             except:
                 iso_score = 0
             result1['iso_score'] = iso_score
             result1['mz_error_abs'] = result1['mz_error'].abs()
             result2 = result1.loc[:, ['ik', 'rt_error', 'mz_error',
-                                             'mz_opt_error','iso_score','mz_error_abs']]
-            result2 = result2.sort_values(by = ['mz_error_abs','iso_score'],ascending = [True,False])
-            result2 = result2.drop('mz_error_abs',axis = 1)
+                                      'mz_opt_error', 'iso_score', 'mz_error_abs']]
+            result2 = result2.sort_values(by=['mz_error_abs', 'iso_score'], ascending=[True, False])
+            result2 = result2.drop('mz_error_abs', axis=1)
             result_str = str(result2.iloc[0, :].T.to_dict())
             unique.loc[i, 'rt_match_result'] = result_str
 
         else:
             unique.loc[i, 'rt_match_result'] = str([])
     return unique
 
@@ -2457,14 +2475,15 @@
         for p_value1 in p_values_columns:
             df = df[df[p_value1] < p_value].reset_index(drop=True)
         # other parameters
         df = df[(df.intensity > i_threshold) & (df.area > area_threshold)].reset_index(drop=True)
 
         df.to_excel(files[i].replace('.xlsx', '_filter.xlsx'))
 
+
 def summarize_results(df, suspect_list=None, db_toxicity=None,
                       rt_matched_column='rt_match_result', matched_DDA_column='match_result_DDA',
                       matched_DIA_column='match_result_DIA', best_matched_DDA_column='best_results_DDA',
                       best_matched_DIA_column='best_results_DIA', MS2_spec_column='MS2_spectra'):
     '''
     The function is designed to collect identified features and ignore unidentified ones, resulting in a dataframe with the relevant information. In order to achieve this, the function requires three input dataframes: a suspect list from the Norman network, an ecotoxicity database from the Norman network, and a compound's category excel.When the function is used, it will extract the name, smile, CAS number, categories, and toxicity data for each identified feature. This information is then compiled into a new dataframe, which includes only the identified features and their associated data. By using this function, users can easily extract and organize the relevant information for identified features, without having to manually sift through large amounts of data.
     Args:
@@ -2489,15 +2508,15 @@
 
     exp_columns = ['new_index', 'rt', 'mz', 'intensity', 'S/N', 'area', 'mz_opt', 'frag_DIA', 'iso_distribution',
                    'resolution', MS2_spec_column]
     assured_columns = [i for i in exp_columns if i in df.columns]
 
     sorted_columns = ['new_index', 'name', 'formula', 'CAS', 'ik', 'Smile', 'rt', 'mz', 'intensity', 'iso_distribution',
                       'S/N', 'area', 'frag_DIA', 'mz_opt', 'resolution', 'MS2_spectra', 'rt_error', 'ms1_error',
-                      'ms1_opt_error', 'match_num','iso_score',
+                      'ms1_opt_error', 'match_num', 'iso_score',
                       'match_percent', 'match_info', 'MS2 mode', 'source', 'source info', 'Norman_SusDat_ID',
                       'Lowest PNEC Freshwater [µg//l]', 'Lowest PNEC Marine water [µg//l]',
                       'Lowest PNEC Sediments [µg//kg dw]', 'Lowest PNEC Biota (fish) [µg//kg ww]']
     final_result_all = []
     for i in tqdm(range(len(df)), desc='Summarizing matched result'):
 
         cmp = df.loc[i, assured_columns]
@@ -2578,14 +2597,15 @@
             final_result_all.append(final_result2)
     final_result_all_df = pd.concat(final_result_all, axis=1).T
     sorted_columns1 = [i for i in sorted_columns if i in final_result_all_df.columns]
 
     final_result_all_df1 = final_result_all_df.loc[:, sorted_columns1]
     return final_result_all_df1
 
+
 def summarized_results_concat(path, all_name_index, mode):
     """
     Summarizes the results from multiple sample sets with specific ESI polarities, and returns a consolidated
     dataframe with unique values (based on site and compound).
 
     Args:
         path (str): The file path for the summarized result files.
@@ -2859,15 +2879,14 @@
                 result_df = result_df.sort_values(by='same_frag_num', ascending=False).reset_index(drop=True)
 
             with ExcelWriter(result_excel_file) as writer:
                 cmp_result.to_excel(writer, sheet_name='Original Data')
                 result_df.to_excel(writer, sheet_name='DDA_parent_products_analysis')
 
 
-
 def formula_to_distribution(formula, adducts='+H', num=3):
     """
     Generate the isotope distribution for a given molecular formula and ion adduct.
 
     Args:
         formula (str): The molecular formula, e.g., 'C13H13N3'.
         adducts (str): The ion adduct, '+H' or '-H','+','-'.
@@ -2881,15 +2900,15 @@
         formula_to_distribution('C13H13N3', '+H', 3)
         (array([198.1072, 199.11  , 200.1129]), array([100. ,  41.5,  10.5]))
     """
 
     f = Formula(formula)
     a = f.spectrum()
     mz_iso, i_iso = np.array([a for a in a.values()]).T
-    i_iso = i_iso / max(i_iso)*100
+    i_iso = i_iso / max(i_iso) * 100
     if adducts == '+H':
         mz_iso += 1.00727647
     elif adducts == '-H':
         mz_iso -= 1.00727647
     elif adducts == '-':
         mz_iso += 0.00054858
     elif adducts == '+':
@@ -3008,16 +3027,16 @@
 
     # Process each file and save output as a new Excel file.
     for file in tqdm(files):
         df = pd.read_excel(file)
         df1 = remove_adducts(df, mode=mode)
         df1.to_excel(file.replace('.xlsx', '_removing_adducts.xlsx'))
 
-        
-def DDA_to_DIA_result(path,company,profile):
+
+def DDA_to_DIA_result(path, company, profile):
     """
   This function processes DDA (Data-Dependent Acquisition) data in mzML format 
   and integrates the results into existing Excel files containing unique company information.
 
   Args:
       path (str): Path to the directory containing mzML files and Excel files.
       company (str): Name of the company associated with the data.
@@ -3032,18 +3051,18 @@
    """
     # 如果有DDA，将DDA数据加入到excel里
     files_excel = glob(os.path.join(path, '*.xlsx'))
     unique_cmps = [file for file in files_excel if 'unique_cmps' in os.path.basename(file)]
     files_mzml_DDA = [file for file in glob(os.path.join(path, '*.mzML')) if 'DDA' in file]
     num = 1
     for file in files_mzml_DDA:
-        ms1,ms2 = sep_scans(file,company)
-        df2 = gen_DDA_ms2_df(ms1,ms2, i_threshold=0, profile=profile, opt=False, more_info = False,message = f'No.{num} ')
+        ms1, ms2 = sep_scans(file, company)
+        df2 = gen_DDA_ms2_df(ms1, ms2, i_threshold=0, profile=profile, opt=False, more_info=False, message=f'No.{num} ')
         name = os.path.basename(file).replace('-DDA', '').replace('_DDA', '').replace('.mzML', '')  # 获得DDA文件的特征名称
-        num +=1
+        num += 1
         for file_excel in unique_cmps:
             if name in os.path.basename(file_excel):
                 df1 = pd.read_excel(file_excel)
                 for i in range(len(df1)):
                     rt, mz = df1.loc[i, ['rt', 'mz']]
                     df_frag = df2[(df2['precursor'] >= mz - 0.015) & (df2['precursor'] <= mz + 0.015)
                                   & (df2['rt'] >= rt - 0.1) & (df2['rt'] <= rt + 0.1)]
@@ -3051,23 +3070,25 @@
                         df1.loc[i, 'frag_DDA'] = str([])
                     else:
                         s_ms2_info = df_frag.iloc[np.argmin(abs(df_frag['rt'].values - rt))]
                         ms2_rt = df_frag['rt'].values[np.argmin(abs(df_frag['rt'].values - rt))]
                         s_ms2 = pd.Series(data=s_ms2_info['intensity'], index=s_ms2_info['frag'], name=ms2_rt)
                         df1.loc[i, 'frag_DDA'] = str(list(s_ms2.index))
                         df1.loc[i, 'MS2_spec_DDA'] = str(s_ms2.astype(int))
-                df1.to_excel(file_excel)  
+                df1.to_excel(file_excel)
+
 
 """
 ========================================================================================================
 2. Swath data process
 ========================================================================================================
 """
 
-def swath_window_checking(file, precursor_ion_start_mass=99.5,mz_overlap = 1):
+
+def swath_window_checking(file, precursor_ion_start_mass=99.5, mz_overlap=1):
     """
     Analyzes the precursor ion windows in a given mzML file. This function separates MS1 and MS2 scans, 
     determines unique precursor ions, and calculates the mass spectrum range for each precursor ion window 
     based on the starting mass and overlap values.
 
     Parameters:
     - file (str): Path to the mzML file containing mass spectrometry data.
@@ -3078,23 +3099,23 @@
     Returns:
     - dict: A dictionary mapping each precursor ion to its corresponding mass spectrum range [start, end].
 
     Note:
     The function first calculates the window size for the initial precursor ion manually and then iteratively for the remaining ions, 
     considering the specified overlap.
     """
-    
+
     ms1, ms2 = sep_scans(file, 'AB')  # 分离ms1和ms2
     # 1. 获得所有selected precursors
     all_precursors = []
     for scan in ms2:
         all_precursors.append(round(scan.selected_precursors[0]['mz'], 1))
     precursors = list(set(all_precursors))
     precursors = sorted(precursors)
-    
+
     # ======计算窗口大小=============
 
     precursor = precursors[0]
     # 第一个要手动
     ms_range = {}
     next_gap = precursors[0] - precursor_ion_start_mass
     ms_range[precursor] = [precursor - next_gap, precursor + next_gap]
@@ -3103,15 +3124,14 @@
         next_start_mz = round(end_mz - mz_overlap, 1)
         next_gap = precursor - next_start_mz
         end_mz = round(precursor + next_gap, 1)
         ms_range[precursor] = [next_start_mz, end_mz]
     return ms_range
 
 
-
 def one_step_process_swath(path, company, profile=True,
                            control_group=['lab_blank', 'methanol'],
                            precursor_ion_start_mass=99.5,
                            filter_type=1,
                            peak_width=2,
                            threshold=15,
                            i_threshold=500,
@@ -3152,14 +3172,15 @@
                                      profile=profile,
                                      peak_width=peak_width,
                                      threshold=threshold,
                                      i_threshold=i_threshold,
                                      SN_threshold=SN_threshold,
                                      mz_overlap=mz_overlap,
                                      rt_error=rt_error, message=f'No. {j + 1} : ')
+
         swath_result.to_excel(file.replace('.mzML', '.xlsx'))
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
 
@@ -3178,33 +3199,32 @@
     print('Third process...')
     print('============================================================================')
     print('                                                                            ')
 
     fold_change_filter(path, control_group=control_group, filter_type=filter_type)
 
 
-
 def swath_process(file, precursor_ion_start_mass=99.5, profile=True, peak_width=2,
-                  threshold=15, i_threshold=500, SN_threshold=3, mz_overlap=1, rt_error=0.05, message = ''):
+                  threshold=15, i_threshold=500, SN_threshold=3, mz_overlap=1, rt_error=0.05, message=''):
     """
     Processing swath-ms data and return a dataframe with all informations
     :param file: file in mzml format
     :param precursor_ion_start_mass: The starting point of the mass spectrum range for the sequential analysis of mass windows.
     :param profile: if profile: True, Centroid: False
     :param threshold: peak picking threshold
     :param i_threshold: intensity threshold
     :param SN_threshold: singal to noise threshold
     :param mz_overlap: MS2 window overlap
     :param rt_error: rt_error
     :return: A dataframe with peak informations
     """
-    ms1, ms2 = sep_scans(file, 'AB',message = message)  # 分离ms1和ms2
+    ms1, ms2 = sep_scans(file, 'AB', message=message)  # 分离ms1和ms2
 
     peak_all_ms1 = split_peak_picking(ms1, profile=profile, threshold=threshold, peak_width=peak_width,
-                                      i_threshold=i_threshold, SN_threshold=SN_threshold,message = message)
+                                      i_threshold=i_threshold, SN_threshold=SN_threshold, message=message)
 
     # 1. 获得所有selected precursors
     all_precursors = []
     for scan in ms2:
         all_precursors.append(round(scan.selected_precursors[0]['mz'], 1))
     precursors = list(set(all_precursors))
     precursors = sorted(precursors)
@@ -3223,19 +3243,18 @@
 
     # 4. 开始分批提取峰
     all_peak_all = {}
     for k, v in all_data.items():
         message1 = f'Processing m/z: {k}:'
         message2 = message + message1
         x = locals()[v]
-        peak_all = split_peak_picking_swath(x, k, profile=profile, 
-                                            i_threshold=i_threshold, peak_width=peak_width, message = message2)
+        peak_all = split_peak_picking_swath(x, k, profile=profile,
+                                            i_threshold=i_threshold, peak_width=peak_width, message=message2)
         all_peak_all[k] = peak_all
 
-
     # ======计算窗口大小=============
 
     precursor = precursors[0]
     # 第一个要手动
     ms_range = {}
     next_gap = precursors[0] - precursor_ion_start_mass
     ms_range[precursor] = [precursor - next_gap, precursor + next_gap]
@@ -3243,15 +3262,15 @@
     for precursor in precursors[1:]:
         next_start_mz = round(end_mz - mz_overlap, 1)
         next_gap = precursor - next_start_mz
         end_mz = round(precursor + next_gap, 1)
         ms_range[precursor] = [next_start_mz, end_mz]
 
     # ======将提取的峰赋值=============
-    for i in tqdm(range(len(peak_all_ms1)),desc = 'Assign the extracted peaks',leave = False):
+    for i in tqdm(range(len(peak_all_ms1)), desc='Assign the extracted peaks', leave=False):
         rt, mz = peak_all_ms1.loc[i, ['rt', 'mz']]
         for k, v in ms_range.items():
             if (mz <= v[1] - 0.5) & (mz > v[0] + 0.5):
                 scan_index = k
                 break
         target_s = all_peak_all[scan_index]
         if len(target_s) == 0:
@@ -3261,18 +3280,21 @@
         else:
             target_s_df = target_s[(target_s['rt'] > rt - rt_error) & (target_s['rt'] < rt + rt_error)]
             mz2, intensity2 = target_s_df['mz'].values, target_s_df['intensity'].values
             frag_s = pd.Series(data=intensity2, index=mz2)
             peak_all_ms1.loc[i, 'frag_swath'] = str(list(mz2))
             peak_all_ms1.loc[i, 'MS2_spectra_swath'] = str(frag_s)
             peak_all_ms1.loc[i, 'MS2_spectra_swath_dict'] = str(frag_s.to_dict())
+    peak_all_ms1 = identify_isotopes(peak_all_ms1)
     return peak_all_ms1
 
+
 def split_peak_picking_swath(ms1, highest_mz, profile=True, split_n=20, threshold=15, peak_width=2, i_threshold=1000,
-                             SN_threshold=5, noise_threshold=0, rt_error_alignment=0.05, mz_error_alignment=0.015,message = ''):
+                             SN_threshold=5, noise_threshold=0, rt_error_alignment=0.05, mz_error_alignment=0.015,
+                             message=''):
     def target_spec1(spec, target_mz, width=0.04):
         """
         :param spec: spec generated from function spec_at_rt()
         :param target_mz: target mz for inspection
         :param width: width for data points
         :return: new spec and observed mz
         """
@@ -3281,39 +3303,41 @@
         new_spec = spec.iloc[index_left:index_right].copy()
         new_spec[target_mz - width] = 0
         new_spec[target_mz + width] = 0
         new_spec = new_spec.sort_index()
         return new_spec
 
     if profile is True:
-        peaks_index = [[i, scipy.signal.find_peaks(ms1[i].i.copy())[0]] 
-                       for i in tqdm(range(len(ms1)),desc = f'{message}Loading Data',leave = False)]
+        peaks_index = [[i, scipy.signal.find_peaks(ms1[i].i.copy())[0]]
+                       for i in tqdm(range(len(ms1)), desc=f'{message}Loading Data', leave=False)]
         raw_info_centroid = {
             round(ms1[i].scan_time[0], 3): pd.Series(data=ms1[i].i[peaks], index=ms1[i].mz[peaks].round(4),
-                                                     name=round(ms1[i].scan_time[0], 3)) for i, peaks in 
+                                                     name=round(ms1[i].scan_time[0], 3)) for i, peaks in
             tqdm(peaks_index, desc=f'{message}Convert to Centroid', leave=False, colour='Green')}
         raw_info_profile = {round(ms1[i].scan_time[0], 3):
                                 pd.Series(data=ms1[i].i, index=ms1[i].mz.round(4), name=round(ms1[i].scan_time[0], 3))
                             for i in tqdm(range(len(ms1)), desc=f'{message}Recording raw profile info', leave=False,
                                           colour='Green')}
         data = []
-        for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking re-index data', leave = False,colour = 'Green'):
+        for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking re-index data', leave=False,
+                         colour='Green'):
             df = v.to_frame().reset_index()
             df = df.sort_values(['index', k])
             df['index'] = np.round(df['index'].values, 3)
             df = df.drop_duplicates('index', keep='last')
             s = pd.Series(df[k].values, index=df['index'], name=k)
             data.append(s)
     else:
         raw_info_centroid = {round(ms1[i].scan_time[0], 3): pd.Series(
             data=ms1[i].i, index=ms1[i].mz.round(4), name=round(ms1[i].scan_time[0], 3)) for i in
             tqdm(range(len(ms1)), desc=f'{message}Loading data', leave=False,
-                                          colour='Green')}
+                 colour='Green')}
         data = []
-        for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking re-index data', leave = False,colour = 'Green'):
+        for k, v in tqdm(raw_info_centroid.items(), desc=f'{message}Checking re-index data', leave=False,
+                         colour='Green'):
             df = v.to_frame().reset_index()
             df = df.sort_values(['index', k])
             df['index'] = np.round(df['index'].values, 3)
             df = df.drop_duplicates('index', keep='last')
             s = pd.Series(df[k].values, index=df['index'], name=k)
             data.append(s)
 
@@ -3321,15 +3345,15 @@
     # 定义变量名称
     all_data = []
     for j in range(split_n):
         name = 'a' + str(j + 1)
         locals()[name] = []
     # 对series进行切割
     ms_increase = int(1000 / split_n)
-    for i in tqdm(range(len(data)), desc=f'{message}Split series', leave = False, colour = 'Green'):
+    for i in tqdm(range(len(data)), desc=f'{message}Split series', leave=False, colour='Green'):
         s1 = data[i]
         low, high = 50, 50 + ms_increase
         for j in range(split_n):
             name = 'a' + str(j + 1)
             if low > highest_mz:
                 break
             locals()[name].append(s1[(s1.index < high) & (s1.index >= low) & (s1.index > noise_threshold)])
@@ -3339,30 +3363,30 @@
     # 将所有数据合并到all_data里
     for j in range(split_n):
         name = 'a' + str(j + 1)
         all_data.append(locals()[name])
 
     # 开始分段提取
     all_peak_all = []
-    for i in tqdm(range(len(all_data)), desc = f'{message}Peak_picking',leave = False, colour = 'Green'):
+    for i in tqdm(range(len(all_data)), desc=f'{message}Peak_picking', leave=False, colour='Green'):
         data1 = all_data[i]
         if len(data1) == 0:
             pass
         else:
             df1 = pd.concat(data1, axis=1)
 
             df1 = df1.fillna(0)
             if len(df1) == 0:
                 pass
             else:
                 highest_mz = df1.index.values.max()
                 peak_all = peak_picking(df1, isotope_analysis=False, threshold=threshold, peak_width=peak_width,
                                         i_threshold=i_threshold, SN_threshold=SN_threshold,
                                         rt_error_alignment=rt_error_alignment,
-                                        mz_error_alignment=mz_error_alignment,enable_progress_bar=False)
+                                        mz_error_alignment=mz_error_alignment, enable_progress_bar=False)
                 all_peak_all.append(peak_all)
 
     # 避免concat空列表
     if len(all_peak_all) == 0:
         peak_all = pd.DataFrame()
     else:
         peak_all = pd.concat(all_peak_all)
@@ -3377,29 +3401,30 @@
         rt_keys = [raw_info_rts[argmin(abs(np.array(raw_info_rts) - i))] for i in rts]  # 基于上述rt找到ms的时间索引
 
         # 更新质量数据
         if profile is True:
             spec1 = [raw_info_profile[i] for i in rt_keys]  # 获得ms的spec
             mz_result = np.array(
                 [list(evaluate_ms(target_spec1(spec1[i], mzs[i], width=0.04).copy(), mzs[i])) for i in
-                 tqdm(range(len(mzs)),desc=f'{message} Correcting m/z', leave=False)]).T
+                 tqdm(range(len(mzs)), desc=f'{message} Correcting m/z', leave=False)]).T
             mz_obs, mz_opt, resolution = mz_result[0], mz_result[2], mz_result[4]
             mz_opt = [mz_opt[i] if abs(mzs[i] - mz_opt[i]) < 0.02 else mzs[i] for i in range(len(mzs))]  # 去掉偏差大的矫正结果
 
             peak_all.loc[:, ['mz', 'mz_opt', 'resolution']] = np.array([mz_obs, mz_opt, resolution.astype(int)]).T
 
         else:
             spec1 = [raw_info_centroid[i] for i in rt_keys]  # 获得ms的spec
             target_spec = [spec1[i][(spec1[i].index > mzs[i] - 0.015) & (spec1[i].index < mzs[i] + 0.015)] for i in
                            range(len(spec1))]
             mzs_obs = [target_spec[i].index.values[[np.argmax(target_spec[i].values)]][0] for i in
                        range(len(target_spec))]
             peak_all['mz'] = mzs_obs
     return peak_all
 
+
 def swath_frag_extract(ms2, mz, frag, error=50, precursor_ion_start_mass=99.5):
     '''
     Chromatogram extract based on precusor and fragment.
     :param ms2: ms2 from sep_scans
     :param mz: precursor
     :param frag: fragment to extract
     :param error: mass error window
@@ -3560,17 +3585,18 @@
             index_e = np.where((mz_all <= high_mz) & (mz_all >= low_mz))
             eic1 = 0 if len(index_e[0]) == 0 else i_all[index_e[0]].max()
             intensity.append(eic1)
     else:
         rt, intensity = None, None
     return rt, intensity
 
-def precursor_frag_peak_area(files_unique,path_to_mzml, company,rt_error = 0.1,
+
+def precursor_frag_peak_area(files_unique, path_to_mzml, company, rt_error=0.1,
                              ms1_error=0.01, ms2_error=0.015,
-                             precursor_ion_start_mass=99.5,profile = True, mz_overlap = 1,sn_info = False):
+                             precursor_ion_start_mass=99.5, profile=True, mz_overlap=1, sn_info=False):
     """
     Analyzes swath mass spectrometry data to collect peak areas for fragment ions within specified retention time (RT) and precursor ranges. This function integrates multiple processing steps, including generating reference peaks, separating scans from mzML files, and calculating peak areas.
 
     Parameters:
     files_excel (list of str): Paths to Excel files containing swath result data.
     path_to_mzml (str): Directory path where mzML files are stored.
     company (str): The name of the company that manufactured the mass spectrometry instrument (e.g., "Agilent", "Waters", "AB", "Thermo").
@@ -3579,31 +3605,34 @@
     ms2_error (float): The error tolerance for the mass-to-charge ratio (m/z) in MS2. Default is 0.015.
     precursor_ion_start_mass (float): The starting mass of the scan range for precursor ions. Default is 99.5.
     mz_overlap (float): The overlap between the fragment windows in m/z units. Default is 1.
 
     Returns:
     None: This function does not return a value. Instead, it generates Excel files directly, containing the calculated peak areas for each fragment. Each Excel file corresponds to a processed mzML file and is saved in the same directory as the input mzML file.
     """
-    
-    
+
     # 1. 处理获得peak_ref
-    peak_ref = gen_ref_swath(files_unique, rt_error = rt_error,ms1_error=ms1_error, ms2_error=ms2_error)
-    peak_ref = [i for i in peak_ref if i[1]>i[2]]
+    peak_ref = gen_ref_swath(files_unique, rt_error=rt_error, ms1_error=ms1_error, ms2_error=ms2_error)
+    peak_ref = [i for i in peak_ref if i[1] > i[2]]
     peak_ref = np.array(peak_ref)
-    
+
     # 2. 获得每个final_area并导出
-    files_mzml = glob(os.path.join(path_to_mzml,'*.mzML'))
+    files_mzml = glob(os.path.join(path_to_mzml, '*.mzML'))
     for i, file in enumerate(files_mzml):
-        ms1,ms2 = sep_scans(file,company)
-        final_area = peak_checking_area_precursor_frag_swath(peak_ref,ms2,
-                                              precursor_ion_start_mass=precursor_ion_start_mass,profile = profile,
-                                              mz_overlap = mz_overlap, message = f'No. {i+1} :',sn_info=sn_info)
-        final_area.to_excel(file.replace('.mzML','_precursor_frag_final_area.xlsx'))
-        
-def peak_checking_area_precursor_frag_swath(peak_ref,ms2,precursor_ion_start_mass=99.5,profile = True,mz_overlap = 1, name = 'area', message = '',sn_info = False):
+        ms1, ms2 = sep_scans(file, company)
+        final_area = peak_checking_area_precursor_frag_swath(peak_ref, ms2,
+                                                             precursor_ion_start_mass=precursor_ion_start_mass,
+                                                             profile=profile,
+                                                             mz_overlap=mz_overlap, message=f'No. {i + 1} :',
+                                                             sn_info=sn_info)
+        final_area.to_excel(file.replace('.mzML', '_precursor_frag_final_area.xlsx'))
+
+
+def peak_checking_area_precursor_frag_swath(peak_ref, ms2, precursor_ion_start_mass=99.5, profile=True, mz_overlap=1,
+                                            name='area', message='', sn_info=False):
     """
     Collects the peak areas for fragments within specified retention time (RT) and precursor ranges from MS2 scans.
 
     This function processes MS2 scans to extract peak areas of fragments that fall within the RT and precursor mass range defined by the peak reference data. It considers the precursor ion start mass and the overlap between fragment windows to segment the data accurately.
 
     Parameters:
     peak_ref (numpy.ndarray): A numpy array containing reference peak information, including retention time (RT), precursor, and fragments. Typically generated by the function "gen_ref_swath".
@@ -3611,15 +3640,15 @@
     precursor_ion_start_mass (float): The starting mass of the scan range for precursor ions. Default is 99.5.
     mz_overlap (float): The overlap between the fragment windows in mass-to-charge ratio (m/z) units. Default is 1.
     message (str): A message string for progress tracking, used with tqdm for displaying progress. Default is an empty string.
 
     Returns:
     pandas.DataFrame: A DataFrame containing the calculated peak areas for each fragment within the specified RT and precursor ranges. Each row corresponds to a fragment, with columns for retention time, m/z, area, and a raw index combining RT, m/z, and fragment number.
     """
-    
+
     # 1. 获得所有selected precursors
     all_precursors = []
     for scan in ms2:
         all_precursors.append(round(scan.selected_precursors[0]['mz'], 1))
     precursors = list(set(all_precursors))
     precursors = sorted(precursors)
 
@@ -3655,24 +3684,40 @@
     for i, (k, v) in enumerate(items):
         message1 = message + 'ms1 range: ' + str(v) + '  '
         # Check if this is the last item in the dictionary
         if i == len(items) - 1:
             x1 = 0
         # Apply the range condition to peak_ref
         sep_peak_ref = peak_ref[np.where((peak_ref[:, 1] >= v[0] + 0.5) & (peak_ref[:, 1] < v[1] - x1))]
-        sep_peak_ref = np.array(sorted(sep_peak_ref,key = lambda x:x[2]))
-        target_ms2 = locals()['ms'+str(k)]
-
-        ref_all = pd.DataFrame(sep_peak_ref[:,[0,2]])
-        ref_all.columns = ['rt','mz']
-        #. xxxxxx
-        peak_area = peak_checking_area_split(ref_all,target_ms2,name,profile = profile,orbi=True, message = message1,sn_info=sn_info)
+        sep_peak_ref = np.array(sorted(sep_peak_ref, key=lambda x: x[2]))
+        target_ms2 = locals()['ms' + str(k)]
 
-        peak_area.loc[:,'raw_index'] = [str(i[0])+ '_' +str(i[1])+ '_' +str(i[2]) for i in sep_peak_ref]
-        peak_area_all.append(peak_area)
+        # revise 1. 根据sep_peak_ref找到传统的rt_mz pair
+        string_array_fast = np.apply_along_axis(lambda row: "_".join(row.astype(str)), 1, sep_peak_ref)
+        # Adjusting the lambda function to only join the first and last elements of each row
+        string_array_first_last = [i.split('_')[0].rstrip('0').rstrip('.')
+                                   + '_' + i.split('_')[-1].rstrip('0').rstrip('.') for i in string_array_fast]
+
+        raw_index_info = pd.DataFrame(index=string_array_fast, data=string_array_first_last, columns=['short_index'])
+        # revise 2. 根据short_index去提取峰
+        short_info = list(set(raw_index_info['short_index'].values))
+        short_info_values = [[eval(i.split('_')[0]), eval(i.split('_')[1])] for i in short_info]
+        ref_all = pd.DataFrame(short_info_values)
+        ref_all.columns = ['rt', 'mz']
+        peak_area = peak_checking_area_split(ref_all, target_ms2, 'peak_area', profile=profile, orbi=True,
+                                             message=message1, sn_info=sn_info)
+        # revise 3. 映射到raw_index_info
+        raw_index_info['peak area'] = raw_index_info['short_index'].map(peak_area['peak_area'])
+        if sn_info is True:
+            raw_index_info['peak_area_S/N'] = raw_index_info['short_index'].map(peak_area['peak_area_S/N'])
+
+        # 解决nan的情况
+        raw_index_info['peak_area_S/N'] = raw_index_info['peak_area_S/N'].fillna(0)
+        raw_index_info['peak area'] = raw_index_info['peak area'].fillna(0)
+        peak_area_all.append(raw_index_info)
     return pd.concat(peak_area_all)
 
 
 def gen_ref_swath(files_excel, rt_error=0.1, ms1_error=0.01, ms2_error=0.015):
     """
     Generates a reference list from a set of data points considering the given rt, ms1, and ms2 error tolerances.
 
@@ -3681,27 +3726,27 @@
     rt_error (float): The error tolerance for retention time.
     ms1_error (float): The error tolerance for ms1.
     ms2_error (float): The error tolerance for ms2.
 
     Returns:
     list: A list of reference points that meet the error criteria.
     """
-    
+
     # Gather all data points from the files
     pairs_all = [
         [
             [df.loc[i, 'rt'], df.loc[i, 'mz'], frag]
             for i in tqdm(range(len(df)), leave=False, desc='Reading swath frag for alignment')
             for frag, intensity in eval2(df.loc[i, 'MS2_spectra_swath']).items()
         ]
         for file in files_excel
         for df in [pd.read_excel(file)]
     ]
     data = np.vstack(pairs_all)
-    
+
     # Convert data to numpy array
     data = np.array(data)
 
     # Calculate the range for rt, ms1, and ms2
     rt_range = np.ptp(data[:, 0])
     ms1_range = np.ptp(data[:, 1])
     ms2_range = np.ptp(data[:, 2])
@@ -3730,27 +3775,28 @@
         if idx in visited:
             continue
 
         # Find neighbors within a spherical range
         neighbors = tree.query_ball_point(scaled_point, r=max(scaled_rt_tol, scaled_ms1_tol, scaled_ms2_tol))
 
         # Filter neighbors based on actual tolerances
-        filtered_neighbors = [i for i in neighbors if 
+        filtered_neighbors = [i for i in neighbors if
                               abs(data[i, 0] - data[idx, 0]) <= rt_error and
-                              abs(data[i, 1] - data[idx, 1]) <= ms1_error and 
+                              abs(data[i, 1] - data[idx, 1]) <= ms1_error and
                               abs(data[i, 2] - data[idx, 2]) <= ms2_error]
 
         # Mark neighbors as visited
         visited.update(filtered_neighbors)
 
         # Add the first neighbor to the reference list
         reference_list.append(list(data[idx]))
 
     return reference_list
 
+
 def eval2(input_string):
     """
     Converts a string representation of a series back into a pandas Series.
 
     Args:
     input_string (str): The string representation of the series.
 
@@ -3775,16 +3821,14 @@
                 index.append(float(index_value[0]))
                 values.append(float(index_value[1]))
         return pd.Series(values, index=index, dtype='float64')
     except Exception as e:
         raise ValueError(f"Error converting string to series: {e}")
 
 
-
-
 """
 ========================================================================================================
 3. Omics functions
 ========================================================================================================
 """
 
 
@@ -4205,15 +4249,15 @@
 """
 ========================================================================================================
 4. FT-ICRMS data processing
 ========================================================================================================
 """
 
 
-def draw_Van_Krevelen_diagrams(result,name = '',path = None,dpi = 300):
+def draw_Van_Krevelen_diagrams(result, name='', path=None, dpi=300):
     """
     Draws Van Krevelen diagrams to visually classify and compare the elemental composition 
     of organic compounds based on their hydrogen-to-carbon (H/C) and oxygen-to-carbon (O/C) ratios. 
     This function differentiates between compounds containing only carbon, hydrogen, and oxygen (CHO), 
     those also containing nitrogen (CHON), sulfur (CHOS), or both (CHONS), and plots them on the diagram 
     with distinct markers. Additionally, it delineates stoichiometric regions associated with different 
     compound classes such as lipids, proteins, lignins, carbohydrates, etc., on the diagram.
@@ -4241,39 +4285,38 @@
           in organic geochemistry and may not be applicable for other types of data.
         - Ensure the 'result' DataFrame is preprocessed to remove NaN values in 'C' column and contains 
           the necessary elemental columns before calling this function.
     """
 
     # 数据处理
     result1 = result[~result['C'].isna()]
-    CHO = result1[(result1['N'] == 0)&(result1['S'] == 0)]
+    CHO = result1[(result1['N'] == 0) & (result1['S'] == 0)]
     CHO_OC = CHO['O/C']
     CHO_HC = CHO['H/C']
-    CHON = result1[(result1['N'] != 0)&(result1['S'] == 0)]
+    CHON = result1[(result1['N'] != 0) & (result1['S'] == 0)]
     CHON_OC = CHON['O/C']
     CHON_HC = CHON['H/C']
-    CHOS = result1[(result1['N'] == 0)&(result1['S'] != 0)]
+    CHOS = result1[(result1['N'] == 0) & (result1['S'] != 0)]
     CHOS_OC = CHOS['O/C']
     CHOS_HC = CHOS['H/C']
-    CHONS = result1[(result1['N'] != 0)&(result1['S'] != 0)]
+    CHONS = result1[(result1['N'] != 0) & (result1['S'] != 0)]
     CHONS_OC = CHONS['O/C']
     CHONS_HC = CHONS['H/C']
 
-
     # Plotting the image
     fig, ax = plt.subplots()
 
-    ax.scatter(CHO_OC,CHO_HC,color = '#E64B35FF',s = 15,marker = 'o',alpha = 0.8,label = 'CHO')
-    ax.scatter(CHON_OC,CHON_HC,color = '#00A087FF',s = 15,marker = '^',alpha = 0.8,label = 'CHON')
-    ax.scatter(CHOS_OC,CHOS_HC,color = '#F7C530FF',s = 15,marker = 's',alpha = 0.8,edgecolor = None,label = 'CHOS')
-    ax.scatter(CHONS_OC,CHONS_HC,color = '#3C5488FF',s = 15,marker = '*',alpha = 0.8,label = 'CHONS')
-    
+    ax.scatter(CHO_OC, CHO_HC, color='#E64B35FF', s=15, marker='o', alpha=0.8, label='CHO')
+    ax.scatter(CHON_OC, CHON_HC, color='#00A087FF', s=15, marker='^', alpha=0.8, label='CHON')
+    ax.scatter(CHOS_OC, CHOS_HC, color='#F7C530FF', s=15, marker='s', alpha=0.8, edgecolor=None, label='CHOS')
+    ax.scatter(CHONS_OC, CHONS_HC, color='#3C5488FF', s=15, marker='*', alpha=0.8, label='CHONS')
+
     ax.set_title(name)
-    ax.set_xlabel('O/C',size = 15)
-    ax.set_ylabel('H/C',size = 15)
+    ax.set_xlabel('O/C', size=15)
+    ax.set_ylabel('H/C', size=15)
     # Defining the stoichiometric ranges for each compound class
     regions = {
         'Lipids': {'H/C': (1.5, 2.0), 'O/C': (0, 0.3)},
         'Aliphatic/proteins': {'H/C': (1.5, 2.2), 'O/C': (0.3, 0.67)},
         'Lignins/CRAM-like structures': {'H/C': (0.7, 1.5), 'O/C': (0.1, 0.67)},
         'Carbohydrates': {'H/C': (1.5, 2.4), 'O/C': (0.67, 1.2)},
         'Unsaturated hydrocarbons': {'H/C': (0.7, 1.5), 'O/C': (0, 0.1)},
@@ -4305,61 +4348,62 @@
         'Tannins': (0.89, 1.3)
     }
 
     for region, boundaries in regions.items():
         label_coord = label_coordinates[region]
         if region == 'Lignins/CRAM-like structures':
             region = 'Lignins/CRAM-like\n structures'
-        ax.text(label_coord[0], label_coord[1], region, 
-                horizontalalignment='center', verticalalignment='center', 
+        ax.text(label_coord[0], label_coord[1], region,
+                horizontalalignment='center', verticalalignment='center',
                 fontsize=12, color='black', bbox=dict(facecolor='white', alpha=0.45, edgecolor='none'))
 
     # 添加箭头
     x_start = 0.05
     y_start = 2.2
     x_end = 0.05
     y_end = 1.4
     # Calculate the length of the arrow
     x_length = x_end - x_start
     y_length = y_end - y_start
     # Add an arrow to the figure
-    ax.arrow(x_start, y_start, x_length, y_length, head_width=0.02, head_length=0.15, fc='blue', ec='blue',lw = 0.5)
+    ax.arrow(x_start, y_start, x_length, y_length, head_width=0.02, head_length=0.15, fc='blue', ec='blue', lw=0.5)
 
     x_start = 0.6
     y_start = 0.45
     x_end = 0.6
     y_end = 1
     # Calculate the length of the arrow
     x_length = x_end - x_start
     y_length = y_end - y_start
     # Add an arrow to the figure
-    ax.arrow(x_start, y_start, x_length, y_length, head_width=0.02, head_length=0.15, fc='blue', ec='blue',lw = 0.5)
+    ax.arrow(x_start, y_start, x_length, y_length, head_width=0.02, head_length=0.15, fc='blue', ec='blue', lw=0.5)
     x_start = 0.6
     y_start = 0.45
     x_end = 0.74
     y_end = 0.45
     # Calculate the length of the arrow
     x_length = x_end - x_start
     y_length = y_end - y_start
     # Add an arrow to the figure
-    ax.arrow(x_start, y_start, x_length, y_length, head_width=0, head_length=0, fc='blue', ec='blue',lw = 0.8)
+    ax.arrow(x_start, y_start, x_length, y_length, head_width=0, head_length=0, fc='blue', ec='blue', lw=0.8)
 
-    plt.xlim(0,1.2)
-    plt.ylim(0,2.75) 
-    plt.tick_params(labelsize = 12)
+    plt.xlim(0, 1.2)
+    plt.ylim(0, 2.75)
+    plt.tick_params(labelsize=12)
     plt.legend()
     if path is None:
         plt.show()
     else:
-        plt.savefig(path,dpi = dpi)
+        plt.savefig(path, dpi=dpi)
         plt.close('All')
 
-def FT_ICRMS_process(data_input,mode = 'neg',atoms=['C', 'H', 'O', 'N','S'],
-                      atom_n=[[5, 60], [0, 150], [0, 50], [0, 10],[0,3]],mz_error = 1,mz_range = [150,1000],
-                     isotope_i_error = 10,peak_threshold=15, peak_width=2):
+
+def FT_ICRMS_process(data_input, mode='neg', atoms=['C', 'H', 'O', 'N', 'S'],
+                     atom_n=[[5, 60], [0, 150], [0, 50], [0, 10], [0, 3]], mz_error=1, mz_range=[150, 1000],
+                     isotope_i_error=10, peak_threshold=15, peak_width=2):
     """
   This function processes FT-ICR-MS data to identify potential elemental formulas corresponding to the peaks in the raw mass spectrum.
 
   Args:
       data_input: Either a list containing [mzs, intensities] or a string representing a file path to .mzML or .xy data files.
       mode: 'pos' for positive mode electrospray ionization or 'neg' for negative mode.
           (default: 'neg')
@@ -4399,20 +4443,21 @@
                 intensities = result['intensity'].values
             else:
                 raise ValueError("Unsupported data input format.")
         else:
             raise ValueError("Data input is neither a list nor a supported file path.")
     except ValueError as e:
         print(e)
-    
+
     background = np.mean(intensities) * 2.5
-    peak_idx, right, left = peak_finding(list(intensities),threshold=peak_threshold, width=peak_width)
-    target_mzs,target_intensities = mzs[peak_idx],intensities[peak_idx]
+    peak_idx, right, left = peak_finding(list(intensities), threshold=peak_threshold, width=peak_width)
+    target_mzs, target_intensities = mzs[peak_idx], intensities[peak_idx]
+
+    all_atoms = atoms  # 先存一个，避免被改
 
-    all_atoms = atoms # 先存一个，避免被改
     # Function to generate chemical formula for a given row
     def generate_formula(row):
         formula = ''.join([f"{el}{int(row[el]) if row[el] > 1 else ''}" for el in atoms if row[el] > 0])
         return formula
 
     atom_mass_table1 = pd.Series(
         data={'C': 12.000000, 'Ciso': 13.003355, 'N': 14.003074, 'Niso': 15.000109, 'O': 15.994915, 'H': 1.007825,
@@ -4433,48 +4478,52 @@
 
     # Converting tuples back to lists (for consistency with the input format)
     atoms1, atom_n1 = list(atoms1), list(atom_n1)
     atoms2, atom_n2 = list(atoms2), list(atom_n2)
 
     # step 2. 获得所有的组合
 
-    atoms=atoms1
-    atom_n=atom_n1
+    atoms = atoms1
+    atom_n = atom_n1
     # sort element list
     elements_sorted_list = ['C', 'H', 'O', 'N', 'S', 'Cl', 'Br', 'P', 'F', 'K', 'Na', 'Ciso', 'D', 'Oiso', 'Niso',
                             'Siso']
 
     atom_indices = {atom: i for i, atom in enumerate(atoms)}
-    sorted_atoms_and_ranges = sorted([(atom, atom_n[atom_indices[atom]]) for atom in atoms if atom in elements_sorted_list], key=lambda x: elements_sorted_list.index(x[0]))
+    sorted_atoms_and_ranges = sorted(
+        [(atom, atom_n[atom_indices[atom]]) for atom in atoms if atom in elements_sorted_list],
+        key=lambda x: elements_sorted_list.index(x[0]))
     atoms, atom_n = zip(*sorted_atoms_and_ranges)
     # generate ranges
     ranges = [range(n[0], n[1] + 1) for n in atom_n]
     # generate patterns_list, this process is very fast
     patterns = np.array(list(itertools.product(*ranges)))
 
     # step3. 去除不太可能的
     # 条件1: H,O,N数量必须≥C
     # 条件2: H,O,N数量必须<2n+2 C
     # 条件3：N_count <= 1.3 * C_coun
     # 条件4：O_count <= 1.3 * C_coun
     # 条件5: O_count <= 1.2 * C_count
     # 条件6: H_count >= C_count + N_count/2 + O_count/2
-    patterns1 = patterns[(patterns[:,0] <= patterns[:,1] +patterns[:,2]+patterns[:,3])&(
-        2*patterns[:,0]+2 >= patterns[:,1] +patterns[:,2]+patterns[:,3]*2)&(
-        1.3* patterns[:,0]>= patterns[:,3])&(1.2* patterns[:,0]>= patterns[:,2])]
+    patterns1 = patterns[(patterns[:, 0] <= patterns[:, 1] + patterns[:, 2] + patterns[:, 3]) & (
+            2 * patterns[:, 0] + 2 >= patterns[:, 1] + patterns[:, 2] + patterns[:, 3] * 2) & (
+                                 1.3 * patterns[:, 0] >= patterns[:, 3]) & (1.2 * patterns[:, 0] >= patterns[:, 2])]
 
     # step 4. 添加剩下的
-    atoms=atoms2
-    atom_n=atom_n2
+    atoms = atoms2
+    atom_n = atom_n2
     # sort element list
     elements_sorted_list = ['C', 'H', 'O', 'N', 'S', 'Cl', 'Br', 'P', 'F', 'K', 'Na', 'Ciso', 'D', 'Oiso', 'Niso',
                             'Siso']
 
     atom_indices = {atom: i for i, atom in enumerate(atoms)}
-    sorted_atoms_and_ranges = sorted([(atom, atom_n[atom_indices[atom]]) for atom in atoms if atom in elements_sorted_list], key=lambda x: elements_sorted_list.index(x[0]))
+    sorted_atoms_and_ranges = sorted(
+        [(atom, atom_n[atom_indices[atom]]) for atom in atoms if atom in elements_sorted_list],
+        key=lambda x: elements_sorted_list.index(x[0]))
     atoms, atom_n = zip(*sorted_atoms_and_ranges)
     # generate ranges
     ranges = [range(n[0], n[1] + 1) for n in atom_n]
     # generate patterns_list, this process is very fast
     patterns2 = np.array(list(itertools.product(*ranges)))
 
     # Step 5:整合在一起,生成最终的final_pattern
@@ -4482,94 +4531,94 @@
     for pattern in patterns2:
         new_array = np.hstack((patterns1, pattern.reshape(1, -1).repeat(len(patterns1), axis=0)))
         all_pattern.append(new_array)
     final_pattern = np.vstack(all_pattern)
 
     # Step 6: 计算所有的值
     mzs = 0
-    atoms = atoms1+atoms2
+    atoms = atoms1 + atoms2
     for i in range(len(atoms)):
         mzs += final_pattern[:, i] * atom_mass_table1[atoms[i]]
-    mzs = mzs+atom_mass_table1['e'] if mode == 'neg' else mzs-atom_mass_table1['e'] # mzs 特指所有可能的质量
+    mzs = mzs + atom_mass_table1['e'] if mode == 'neg' else mzs - atom_mass_table1['e']  # mzs 特指所有可能的质量
 
     # Step 7: 继续精简数据
-    mz_range_idx = np.where(mzs[(mzs>mz_range[0])&(mzs<mz_range[1])])
+    mz_range_idx = np.where(mzs[(mzs > mz_range[0]) & (mzs < mz_range[1])])
     final_pattern1 = final_pattern[mz_range_idx]
     mzs1 = mzs[mz_range_idx]
 
     # step 8: 开始做计算
     df_all = []
     hetero_atoms = [atom for atom in atoms if atom not in ['C', 'H', 'O', 'N']]
-    for i in tqdm(range(len(target_mzs)), desc='Calculating the formula',leave = True):
+    for i in tqdm(range(len(target_mzs)), desc='Calculating the formula', leave=True):
         target_mz = target_mzs[i]
         target_intensity = target_intensities[i]
         index = np.where((mzs1 > target_mz * (1 - mz_error * 1e-6)) & (mzs1 < target_mz * (1 + mz_error * 1e-6)))
         if len(index[0]) == 0:
-            s1 = pd.Series({'obs_mass':target_mz,'intensity':target_intensity})
+            s1 = pd.Series({'obs_mass': target_mz, 'intensity': target_intensity})
             df_all.append(s1)
         else:
             arr1 = final_pattern1[index]
             exact_mass = mzs1[index]
             # Reshape the array to make it 2-dimensional
             arr_2d = arr1.reshape(arr1.shape[0], -1)
             # Creating the DataFrame
             df = pd.DataFrame(arr_2d, columns=atoms)
             df['obs_mass'] = target_mz
             df['exact_mass'] = exact_mass
             df['intensity'] = target_intensity
             df['hetero_atom_num'] = df[hetero_atoms].sum(axis=1)
             df['mz_error'] = round((df['exact_mass'] - target_mz) / target_mz * 1e6, 2)
-            df['mz_error_abs'] =df['mz_error'].abs()
+            df['mz_error_abs'] = df['mz_error'].abs()
             df['formula'] = df.apply(generate_formula, axis=1)
-            df1 = df.sort_values(by = ['hetero_atom_num','mz_error_abs']).reset_index(drop = True)
+            df1 = df.sort_values(by=['hetero_atom_num', 'mz_error_abs']).reset_index(drop=True)
             df_all.append(df1.iloc[0])
-    result = pd.concat(df_all,axis=1).T.sort_values(by = 'intensity',ascending = False).reset_index(drop=True)
-    
-    #开始处理同位素
-    column_names = [i for i in result.columns if i not in ['obs_mass','intensity', 'exact_mass',
+    result = pd.concat(df_all, axis=1).T.sort_values(by='intensity', ascending=False).reset_index(drop=True)
+
+    # 开始处理同位素
+    column_names = [i for i in result.columns if i not in ['obs_mass', 'intensity', 'exact_mass',
                                                            'mz_error', 'mz_error_abs']]
     isotope_idx = []
     isotope_df = []
-    for i in tqdm(range(len(result)),desc = 'Processing isotope',leave = True):
-        formula = result.loc[i,'formula']
-        obs_mz = result.loc[i,'obs_mass']
-        obs_intensity = result.loc[i,'intensity']
-        basic_info = result.loc[i,column_names].values # 为了给后面匹配到的赋值
-        if type(formula) is str: # 如果有分子式，去找它的同位素
+    for i in tqdm(range(len(result)), desc='Processing isotope', leave=True):
+        formula = result.loc[i, 'formula']
+        obs_mz = result.loc[i, 'obs_mass']
+        obs_intensity = result.loc[i, 'intensity']
+        basic_info = result.loc[i, column_names].values  # 为了给后面匹配到的赋值
+        if type(formula) is str:  # 如果有分子式，去找它的同位素
             # 先生成其同位素信息
-            isotopes,distribution = formula_to_distribution(formula,adducts = '-' if mode == 'neg' else '+',num = 5)
-            for j,iso_mz in enumerate(isotopes):
-                if abs(1e6*(obs_mz-iso_mz)/iso_mz)<1:
+            isotopes, distribution = formula_to_distribution(formula, adducts='-' if mode == 'neg' else '+', num=5)
+            for j, iso_mz in enumerate(isotopes):
+                if abs(1e6 * (obs_mz - iso_mz) / iso_mz) < 1:
                     pass
                 else:
-                    result1 = result[(result['obs_mass']<iso_mz*(1+mz_error*1e-6))&(
-                        result['obs_mass']>iso_mz*(1-mz_error*1e-6))].copy()
+                    result1 = result[(result['obs_mass'] < iso_mz * (1 + mz_error * 1e-6)) & (
+                            result['obs_mass'] > iso_mz * (1 - mz_error * 1e-6))].copy()
 
-                    if len(result1)!=0:
-                        iso_intensity = result1['intensity'].values[0] # isotope的intensity
-                        iso_ratio = round(iso_intensity/obs_intensity*100,1)
-                        iso_ratio_error = iso_ratio-distribution[j] #获得同位素峰响应的偏差
+                    if len(result1) != 0:
+                        iso_intensity = result1['intensity'].values[0]  # isotope的intensity
+                        iso_ratio = round(iso_intensity / obs_intensity * 100, 1)
+                        iso_ratio_error = iso_ratio - distribution[j]  # 获得同位素峰响应的偏差
                         obs_mass_iso_candi = result1['obs_mass'].values[0]
                         idx1 = result1.index.values
-                        result1.loc[:,column_names] =basic_info # 先填充其他信息
-                        result1.loc[idx1,'exact_mass'] = iso_mz
-                        real_mass_error = round((iso_mz - obs_mass_iso_candi)/obs_mass_iso_candi*1e6,2)
-                        result1.loc[idx1,'mz_error'] = real_mass_error
-                        result1.loc[idx1,'mz_error_abs'] = abs(real_mass_error)
-                        result1.loc[idx1,'isotope_peak'] = 'yes'
-                        result1.loc[idx1,'iso_ratio_error(%)'] = iso_ratio_error
+                        result1.loc[:, column_names] = basic_info  # 先填充其他信息
+                        result1.loc[idx1, 'exact_mass'] = iso_mz
+                        real_mass_error = round((iso_mz - obs_mass_iso_candi) / obs_mass_iso_candi * 1e6, 2)
+                        result1.loc[idx1, 'mz_error'] = real_mass_error
+                        result1.loc[idx1, 'mz_error_abs'] = abs(real_mass_error)
+                        result1.loc[idx1, 'isotope_peak'] = 'yes'
+                        result1.loc[idx1, 'iso_ratio_error(%)'] = iso_ratio_error
                         # 搜集信息
-                        if iso_ratio_error< isotope_i_error:
+                        if iso_ratio_error < isotope_i_error:
                             if idx1 not in isotope_idx:
                                 isotope_idx.append(idx1)
                                 isotope_df.append(result1)
-    result_remaining = result.drop(index = np.hstack(isotope_idx),axis=0)
+    result_remaining = result.drop(index=np.hstack(isotope_idx), axis=0)
     isotope_result = pd.concat(isotope_df)
-    final_result = pd.concat([result_remaining,isotope_result]).sort_index()
-    
+    final_result = pd.concat([result_remaining, isotope_result]).sort_index()
+
     # 计算其他参数
     final_result['S/N'] = (final_result['intensity'] / background).astype(float).round(2)
     final_result['O/C'] = (final_result['O'] / final_result['C']).astype(float).round(3)
     if mode == 'pos':
         x = 1
     elif mode == 'neg':
         x = -1
@@ -4582,15 +4631,15 @@
                                 - 3 * final_result['N'] - 2 * final_result['O'] - 2 * final_result['S']) / final_result[
                                'C']
     final_result['NOSC'] = final_result['NOSC'].astype(float).round(3)
     AI_denominator = final_result['C'] - 0.5 * final_result['O'] - final_result['S'] - final_result['N']
     AI_numerator = 1 + final_result['C'] - 0.5 * final_result['O'] - final_result['S'] - 0.5 * (final_result['H'] - x)
     AI = AI_numerator / (AI_denominator.sort_values() + 1 * 1e-6)
     final_result['AI'] = AI
-    
+
     return final_result
 
 
 """
 ========================================================================================================
 5. Ion mobility data processing
 ========================================================================================================
@@ -5000,15 +5049,15 @@
     Inchikey, precursors, frags, formulas, smiles, ion_modes, instrument_types, collision_energies = [], [], [], [], [], [], [], []
     cases, pubchem_cids, inchis, total_exact_masses = [], [], [], []
     columns, mass_accuracies, precursor_mzs, precursor_types, ionization_modes = [], [], [], [], []
     kingdoms, superclasses, class1s, subclasses = [], [], [], []
     names = []
     frag_annotations = []
     num = len(json_data)
-    for i in tqdm(range(num),desc = 'Extracting info',leave = True, colour = 'Green'):
+    for i in tqdm(range(num), desc='Extracting info', leave=True, colour='Green'):
         # 信息1:包括分子信息
         info1 = json_data[i]['compound'][0]['metaData']
         ik_info = [x['value'] for x in info1 if x['name'] == 'InChIKey']
         formula_info = [x['value'] for x in info1 if x['name'] == 'molecular formula']
         precursor_info = [x['value'] for x in info1 if x['name'] == 'total exact mass']
         smile_info = [x['value'] for x in info1 if x['name'] == 'SMILES']
         cas_info = [x['value'] for x in info1 if x['name'] == 'cas']  # 新增cas
@@ -5121,17 +5170,14 @@
                  'cas', 'pubchem_cid', 'Inchi', 'total_exact_mass', 'chromatogram column info', 'mass_accuracy (ppm)',
                  'precursor mz', 'precursor_types', 'ionization_mode', 'kingdom', 'superclass', 'class1s', 'subclasses',
                  'names'
                  ])
     return database
 
 
-
-
-
 def calibration(path, mode='internal'):
     """
     Calibrate using internal or external standard method, must have 'all_area_df.xlsx', 'quan_info.xlsx',
     and 'alignment' files.
     :param path: path for excel files
     :param mode: external or internal
     :return: result dataframe
@@ -5488,15 +5534,14 @@
 
     if path is not None:
         bbox_extra_artists = [legend]  # Include legend in the bounding box calculation
         plt.savefig(path, dpi=500, bbox_inches='tight', bbox_extra_artists=bbox_extra_artists)
         plt.close('all')
 
 
-
 def AIF_multi_ce(path, company='Agilent', profile=False, control_group=['Methanol'], collision_energies=[10, 20, 40],
                  filter_type=1, frag_rt_error=0.02, split_n=20, peak_width=2,
                  sat_intensity=False, long_rt_split_n=1, threshold=15, i_threshold=500, SN_threshold=3, orbi=False):
     """
     Processes AIF data when multiple collision energies are present.
 
     Args:
@@ -5596,15 +5641,15 @@
 
     Note:
         The peak_all and peak_all2 dataframes should contain 'rt', 'mz', and 'intensity' columns.
     """
 
     frag_all = []
     spec_all = []
-    for i in tqdm(range(len(peak_all)), desc='Assign DIA MS2 spectrum',leave = False, colour = 'Green'):
+    for i in tqdm(range(len(peak_all)), desc='Assign DIA MS2 spectrum', leave=False, colour='Green'):
         rt = peak_all.loc[i, 'rt']
         df_DIA = peak_all2[(peak_all2['rt'] > rt - frag_rt_error)
                            & (peak_all2['rt'] < rt + frag_rt_error)].sort_values(
             by='intensity', ascending=False)
         # append fragments
         frag = str(list(df_DIA['mz'].values))
         frag_all.append(frag)
@@ -5767,15 +5812,15 @@
     Returns:
     None. Displays or saves a visualization of the molecular network.
     """
 
     # data analysis
     # Identify pairs that contain at least one of the parent compounds
     parent_pairs = data[(data['cmp1'].isin(parent_cmps) | data['cmp2'].isin(parent_cmps)) & (
-                data['same_frag_num'] >= frag_num_for_network)]
+            data['same_frag_num'] >= frag_num_for_network)]
 
     # Extract target compounds from parent pairs
     target_cmp = set(parent_pairs['cmp1'].tolist() + parent_pairs['cmp2'].tolist())
 
     # find their relationships
     target_pairs = data[
         data['cmp1'].isin(target_cmp) & data['cmp2'].isin(target_cmp) & (data['same_frag_num'] >= frag_num_for_network)]
@@ -5938,16 +5983,17 @@
     for q in range(len(checking_list)):
         df_q = pd.DataFrame(locals()['dict' + str(q)])
         final_df = pd.concat([istd_df, df_q], axis=1)
         final_data.append(final_df)
 
     return final_data
 
+
 def formula_prediction(mz, mode, atoms=['C', 'H', 'O', 'N'],
-                       atom_n=[[5, 30], [0, 50], [0, 30], [0, 10]], max_possible_num=2e7, mz_error=5 ):
+                       atom_n=[[5, 30], [0, 50], [0, 30], [0, 10]], max_possible_num=2e7, mz_error=5):
     """
     Predicts the chemical formula based on the provided mz value.
 
     Parameters:
     - mz (float or list/np.ndarray): The mass-to-charge ratio(s). If a list/ndarray is provided, formulae for all values will be predicted.
     - atoms (list): List of chemical elements to consider.
     - atom_n (list): Range for each atom to consider in the prediction.
@@ -5972,19 +6018,21 @@
     def generate_formula(row):
         formula = ''.join([f"{el}{int(row[el]) if row[el] > 1 else ''}" for el in atoms if row[el] > 0])
         return formula
 
     # sort element list
     elements_sorted_list = ['C', 'H', 'O', 'N', 'S', 'Cl', 'Br', 'P', 'F', 'K', 'Na', 'Ciso', 'D', 'Oiso', 'Niso',
                             'Siso']
-    
+
     atom_indices = {atom: i for i, atom in enumerate(atoms)}
-    sorted_atoms_and_ranges = sorted([(atom, atom_n[atom_indices[atom]]) for atom in atoms if atom in elements_sorted_list], key=lambda x: elements_sorted_list.index(x[0]))
+    sorted_atoms_and_ranges = sorted(
+        [(atom, atom_n[atom_indices[atom]]) for atom in atoms if atom in elements_sorted_list],
+        key=lambda x: elements_sorted_list.index(x[0]))
     atoms, atom_n = zip(*sorted_atoms_and_ranges)
-    
+
     # generate ranges
     ranges = [range(n[0], n[1] + 1) for n in atom_n]
     # generate patterns_list, this process is very fast
     patterns_list = list(itertools.product(*ranges))
 
     if len(patterns_list) > max_possible_num:
         raise ValueError(
@@ -5992,15 +6040,15 @@
     else:
         # generate a np.array, this is time-consuming
         patterns = np.array(patterns_list)
 
         mzs = 0
         for i in range(len(atoms)):
             mzs += patterns[:, i] * atom_mass_table1[atoms[i]]
-        mzs = mzs+atom_mass_table1['e'] if mode == 'neg' else mzs-atom_mass_table1['e']
+        mzs = mzs + atom_mass_table1['e'] if mode == 'neg' else mzs - atom_mass_table1['e']
     if isinstance(mz, float):
         target_mz = mz
         index = np.argwhere((mzs > target_mz * (1 - mz_error * 1e-6)) & (mzs < target_mz * (1 + mz_error * 1e-6)))
         if len(index) == 0:
             return pd.DataFrame()
         else:
             arr1 = patterns[index]
@@ -6036,15 +6084,15 @@
                 df1['mz_error'] = round((df1['exact_mass'] - target_mz) / target_mz * 1e6, 2)
                 df1['formula'] = df1.apply(generate_formula, axis=1)
                 possible_formulas.append(df1.loc[0, 'formula'])
                 errors.append(df1.loc[0, 'mz_error'])
         return possible_formulas, errors
     else:
         raise ValueError(f"Unsupported type for mz: {type(mz)}")
-        
+
 
 def convert_db(df, relative_i_threshold=20, source='', source_info=''):
     """
     Convert a Massbank database from Excel to a format compatible with pyhrms.
 
     This function processes an Excel file (previously converted from a JSON file using the JsonToExcel function) to create a database suitable for use in pyhrms. It filters fragment masses based on a relative intensity threshold and adds metadata about the source of the database.
 
@@ -6060,15 +6108,16 @@
     The function processes each compound in the dataframe, filtering out fragment masses that do not meet the specified intensity threshold and are less than the precursor mass minus 5. It also formats ion modes and includes source information in the resulting dataframe.
     """
     iks = df['Inchikey'].value_counts().index.values
     info_all = []
     for ik in tqdm(iks, desc='Collecting MS2 info'):
         df1 = df[df['Inchikey'] == ik].reset_index(drop=True)
         target_df = df1.loc[0, ['Inchikey', 'Precursor', 'Formula', 'Smiles', 'ion_modes']]
-        precursor = eval(df1['Precursor'].iloc[0]) if isinstance(df1['Precursor'].iloc[0],str) else df1['Precursor'].iloc[0]
+        precursor = eval(df1['Precursor'].iloc[0]) if isinstance(df1['Precursor'].iloc[0], str) else \
+        df1['Precursor'].iloc[0]
         # 1. 获得所有该化合物信息
         all_s = []
         for i in range(len(df1)):
             s = pd.Series(eval(df1.loc[i, 'Frag']))
             all_s.append(s)
         all_s_s = pd.concat(all_s)
         # 2. 去掉特别相似的质量，保留响应高的质量
@@ -6085,15 +6134,15 @@
         info_all.append(target_df)
     database = pd.concat(info_all, axis=1).T.reset_index(drop=True)
     database['ion_modes'] = database['ion_modes'].apply(
         lambda a: a.replace('positive', 'pos').replace('negative', 'neg'))
     database['Source'] = source
     database['Source info'] = source_info
     database['rt'] = np.nan
-    database = database.rename(columns = {'ion_modes':'mode'})
+    database = database.rename(columns={'ion_modes': 'mode'})
     return database
 
 
 def get_chemical_name(query, language='both'):
     """
     Fetches the name of a chemical compound given its CAS number, English name, or Chinese name.
     The function supports retrieving the name in Chinese ('cn'), English ('en'), or both ('both').
@@ -6167,14 +6216,15 @@
     element_mass = atom_mass_table[element] * formula_dict.get(element, 0)
 
     # Calculate the mass percentage
     mass_percentage = (element_mass / total_mass) * 100
 
     return mass_percentage
 
+
 def pubchem_search(file, sleep_time=1):
     """
     Searches PubChem database to fill missing information in a given Excel file.
 
     Args:
         file (str): The path to the Excel file.
 
@@ -6187,32 +6237,31 @@
     If a compound's name, formula, or CAS number is missing, the function queries PubChem using
     the provided InChIKey and retrieves the information if available.
 
     Example:
         updated_data = pubchem_search('data.xlsx')
         print(updated_data.head())
     """
-    
-    
+
     import pubchempy as pcp
     df = pd.read_excel(file)
-    
+
     # Ensure these columns exist
     for column in ['name', 'formula', 'CAS', 'Smile']:
         if column not in df.columns:
             df[column] = np.nan
 
     query_num = 0
     response_num = 0
 
     try:
         for i in tqdm(range(len(df)), desc='Searching in pubchem'):
             time.sleep(sleep_time)
             name, formula, cas, smile, ik = df.loc[i, ['name', 'formula', 'CAS', 'Smile', 'ik']]
-            
+
             if pd.isna(name) or pd.isna(formula) or pd.isna(cas) or pd.isna(smile):
                 query_num += 1
                 try:
                     cmp_all = pcp.get_compounds(ik, namespace='inchikey')
                     cmp = cmp_all[0] if cmp_all else None
                     if cmp:
                         synonyms = cmp.synonyms
@@ -6233,18 +6282,20 @@
                         response_num += 1
                 except Exception as e:
                     print(f"Error at index {i}: {e}")
 
     except (KeyboardInterrupt, TimeoutError):
         print("Interrupted! Returning partial results.")
 
-    print(f'Total query number = {query_num}; Total response number = {response_num}, Response rate = {round(response_num / query_num * 100, 2) if query_num else 0} %')
+    print(
+        f'Total query number = {query_num}; Total response number = {response_num}, Response rate = {round(response_num / query_num * 100, 2) if query_num else 0} %')
     return df
 
-def get_correction_factor_waters(file,lock_mass = 556.2771):
+
+def get_correction_factor_waters(file, lock_mass=556.2771):
     """
     Calculate the correction factor for Waters LC-TOF-MS data based on the lockmass value.
 
     Parameters:
     - file (str): Path to the mzML file containing the LC-TOF-MS data.
     - lockmass (float, optional): The lockmass value used for correction. Default values are 
       556.2771 for positive mode and 554.2615 for negative mode.
@@ -6260,30 +6311,29 @@
     """
     run = pymzml.run.Reader(file)
     function_nums = [scan.id_dict['function'] for scan in run]
     func_num_max = max(function_nums)
     lockspray = [scan for scan in run if scan.id_dict['function'] == func_num_max]
     mz_corr_factors = []
     for scan in lockspray:
-        mz,intensity = scan.mz,scan.i
+        mz, intensity = scan.mz, scan.i
         s = pd.Series(data=intensity, index=mz)
         if len(s) == 0:
             pass
         else:
             s1 = s.loc[(s.index > lock_mass - 0.2) & (s.index < lock_mass + 0.2)]
             lockmass_obs = s1.idxmax()
             factor = lock_mass / lockmass_obs
             mz_corr_factors.append(factor)
     factor_median = np.median(mz_corr_factors)
     factor_mean = np.mean(mz_corr_factors)
-    return factor_median,factor_mean
+    return factor_median, factor_mean
 
 
-
-def compare_ms_spectra(spec_db,spec_obs, error = 0.015):
+def compare_ms_spectra(spec_db, spec_obs, error=0.015):
     """
     Calculate the matching score between two mass spectrometry spectra.
 
     This function computes a similarity score based on the overlap and difference in intensity values
     between a reference spectrum (spec_db) and an observed spectrum (spec_obs) within a specified
     mass error tolerance (error).
 
@@ -6295,114 +6345,110 @@
     Returns:
         float: A similarity score ranging from 0 (no match) to 1 (perfect match) between the two spectra.
     """
     s1 = spec_db
     s2 = spec_obs
     s_all = []
     for index in s1.index:
-        s = s2[(s2.index>index-error)&(s2.index<index+error)]
-        if len(s)!=0:
-            s_all.append(s.sort_values(ascending = False).head(1))
+        s = s2[(s2.index > index - error) & (s2.index < index + error)]
+        if len(s) != 0:
+            s_all.append(s.sort_values(ascending=False).head(1))
         else:
-            s_all.append(pd.Series(index = [index],data = [0]))
+            s_all.append(pd.Series(index=[index], data=[0]))
     s2_new = pd.concat(s_all)
 
-    s1_nor = s1/max(s1.values)
-    index_max = s2_new.sort_values(ascending =False).head(1).index[0]
-    index_value = s1_nor.values[np.argmin(abs(s1_nor.index.values-index_max))]
-    s2_nor = s2_new/max(s2_new)*index_value if max(s2_new)!=0 else s2_new
-    if max(s2_new)==0:
+    s1_nor = s1 / max(s1.values)
+    index_max = s2_new.sort_values(ascending=False).head(1).index[0]
+    index_value = s1_nor.values[np.argmin(abs(s1_nor.index.values - index_max))]
+    s2_nor = s2_new / max(s2_new) * index_value if max(s2_new) != 0 else s2_new
+    if max(s2_new) == 0:
         score = 0
     else:
-        diff = sum([abs(s1_nor.values[i]-s2_nor.values[i]) for i in range(len(s1_nor))])/s1_nor.sum()
+        diff = sum([abs(s1_nor.values[i] - s2_nor.values[i]) for i in range(len(s1_nor))]) / s1_nor.sum()
         score = 1 - diff
     return score
 
 
-def first_process_ms2(file, company, profile=True, 
-                  i_threshold=200, SN_threshold=3, peak_width=2,threshold=15,
-                  frag_rt_error=0.02, split_n=20, sat_intensity=False, long_rt_split_n=1,
-                  orbi=False, message = ''):
+def first_process_ms2(file, company, profile=True,
+                      i_threshold=200, SN_threshold=3, peak_width=2, threshold=15,
+                      frag_rt_error=0.02, split_n=20, sat_intensity=False, long_rt_split_n=1,
+                      orbi=False, message=''):
     """
     Only for MS2 data
     """
-    
+
     mz_round = 4
-    ms1, ms2 = sep_scans(file, company,message = message)
+    ms1, ms2 = sep_scans(file, company, message=message)
     peak_all = ultimate_peak_picking(ms2, profile=profile, split_n=split_n, i_threshold=i_threshold,
-                                     peak_width=peak_width,threshold=threshold,
+                                     peak_width=peak_width, threshold=threshold,
                                      SN_threshold=SN_threshold, sat_intensity=sat_intensity,
-                                     long_rt_split_n=long_rt_split_n, orbi=orbi,message=message)
+                                     long_rt_split_n=long_rt_split_n, orbi=orbi, message=message)
     file_name = os.path.basename(file)
     peak_selected = identify_isotopes(peak_all)
     peak_selected = remove_unnamed_columns(peak_selected)
     peak_selected.to_excel(file.replace('.mzML', '_ms2_data.xlsx'))
 
-def second_process_ms2(file, ref_all, company, profile=True, long_rt_split_n=1, orbi=False, message = ''):
+
+def second_process_ms2(file, ref_all, company, profile=True, long_rt_split_n=1, orbi=False, message=''):
     """
     Only for MS2 data
     """
     ms_round = 4
-    ms1, ms2 = sep_scans(file, company, message = message)
+    ms1, ms2 = sep_scans(file, company, message=message)
 
     name1 = os.path.basename(file).split('.')[0]
     final_result = ultimate_checking_area(ref_all, ms2, name1, profile=profile,
-                                          rt_overlap=1, long_rt_split_n=long_rt_split_n, orbi=orbi,message = message)
+                                          rt_overlap=1, long_rt_split_n=long_rt_split_n, orbi=orbi, message=message)
     final_result.to_excel(file.replace('.mzML', '_ms2_final_area.xlsx'))
 
-    
 
-def one_step_process_ms2(path, company, profile=True, control_group=['lab_blank', 'methanol'], 
-                     peak_width=2,threshold = 15, filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1, orbi=False):
+def one_step_process_ms2(path, company, profile=True, control_group=['lab_blank', 'methanol'],
+                         peak_width=2, threshold=15, filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1,
+                         orbi=False):
     """
     Only for ms2 data.
 
     """
     print('                                                                            ')
     print('============================================================================')
     print('First process...')
     print('============================================================================')
     print('                                                                            ')
 
     files_mzml = glob(os.path.join(path, '*.mzML'))
-    
+
     files_mzml = [file for file in files_mzml if 'DDA' not in os.path.basename(file)]
-    for j,file in enumerate(files_mzml):
+    for j, file in enumerate(files_mzml):
         first_process_ms2(file, company=company, profile=profile,
-                      peak_width=peak_width,threshold = threshold, split_n=split_n, sat_intensity=sat_intensity,
-                      long_rt_split_n=long_rt_split_n, orbi=orbi, message = f'No. {j+1} : ')
-
+                          peak_width=peak_width, threshold=threshold, split_n=split_n, sat_intensity=sat_intensity,
+                          long_rt_split_n=long_rt_split_n, orbi=orbi, message=f'No. {j + 1} : ')
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
-    
 
-    
     # 第二个过程
     print('                                                                            ')
     print('============================================================================')
     print('Second process...')
     print('============================================================================')
     print('                                                                            ')
-    for j,file in enumerate(files_mzml):
-        second_process_ms2(file, ref_all, company, profile=profile, long_rt_split_n=long_rt_split_n, orbi=orbi,message = f'No. {j+1} ')
-    
+    for j, file in enumerate(files_mzml):
+        second_process_ms2(file, ref_all, company, profile=profile, long_rt_split_n=long_rt_split_n, orbi=orbi,
+                           message=f'No. {j + 1} ')
 
     # 第三个过程, 做fold change filter
     print('                                                                            ')
     print('============================================================================')
     print('Third process...')
     print('============================================================================')
     print('                                                                            ')
 
     fold_change_filter(path, control_group=control_group, filter_type=filter_type)
 
 
-
-
 if __name__ == '__main__':
-    pass  
+    pass
 # %config InlineBackendlineBackend.figure_format ='retina'
 #  plt.rcParams['font.sans-serif'] = 'Times New Roman'  # 设置全局字体
```

## Comparing `pyhrms-0.8.8/pyhrms/__init__.py` & `pyhrms-0.8.9/pyhrms/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from . import pyhrms
 
-__version__ = "0.8.8"
+__version__ = "0.8.9"
 
 __all__ = [
     "sep_scans",
     "gen_df",
     "peak_picking",
     "split_peak_picking",
     "remove_unnamed_columns",
```

